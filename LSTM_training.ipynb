{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LSTM_training.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "5DYTC0KvciyH",
        "N9DMTLO-rvQb"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xry6orXVwMcK"
      },
      "source": [
        "# import and read csv"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H5F6M1JSO5ZJ"
      },
      "source": [
        "%matplotlib inline\n",
        "import numpy as np\n",
        "import matplotlib\n",
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i4s-83veMySh"
      },
      "source": [
        "from numpy.random import seed \n",
        "seed(7)\n",
        "\n",
        "import tensorflow as tf \n",
        "tf.random.set_seed(7)\n",
        "from tensorflow.keras.layers import *\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import BatchNormalization\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4S_hww6xOwp4",
        "outputId": "0ee82ba8-f1dc-42d4-fd43-4c686126c245"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive', force_remount=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eQj17g0NOjvH"
      },
      "source": [
        "embeddings_dict = {}\n",
        "\n",
        "with open(\"/content/gdrive/MyDrive/QuoraQuestions/glove.6B.50d.txt\", 'r') as f:\n",
        "    for line in f:\n",
        "        values = line.split()\n",
        "        word = values[0]\n",
        "        vector = np.asarray(values[1:], \"float32\")\n",
        "        embeddings_dict[word] = vector\n",
        "    f.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nPQx9e_NSnib"
      },
      "source": [
        "path = \"/content/gdrive/MyDrive/QuoraQuestions/cleaned_features.csv\"\n",
        "train_df = pd.read_csv(path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 745
        },
        "id": "jFnCV35CZNki",
        "outputId": "2b3f124a-5db9-4e83-bd4a-0d761a695e50"
      },
      "source": [
        "train_df"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>is_duplicate</th>\n",
              "      <th>qid1</th>\n",
              "      <th>qid2</th>\n",
              "      <th>question1_cleaned</th>\n",
              "      <th>question2_cleaned</th>\n",
              "      <th>cosine_similarity</th>\n",
              "      <th>q1_word_count</th>\n",
              "      <th>q2_word_count</th>\n",
              "      <th>q1char_count</th>\n",
              "      <th>q2char_count</th>\n",
              "      <th>freq_qid1</th>\n",
              "      <th>freq_qid2</th>\n",
              "      <th>common_words_count</th>\n",
              "      <th>total_unique_num_words</th>\n",
              "      <th>tot_words</th>\n",
              "      <th>words_ratio</th>\n",
              "      <th>Simple_Ratio</th>\n",
              "      <th>Partial_Ratio</th>\n",
              "      <th>Token_Sort_Ratio</th>\n",
              "      <th>Token_Set_Ratio</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>what is the step by step guide to invest in sh...</td>\n",
              "      <td>what is the step by step guide to invest in sh...</td>\n",
              "      <td>0.891528</td>\n",
              "      <td>14</td>\n",
              "      <td>12</td>\n",
              "      <td>65</td>\n",
              "      <td>56</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>11</td>\n",
              "      <td>12</td>\n",
              "      <td>23</td>\n",
              "      <td>0.478261</td>\n",
              "      <td>93</td>\n",
              "      <td>100</td>\n",
              "      <td>93</td>\n",
              "      <td>100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>what is the story of kohinoor koh i noor diamond</td>\n",
              "      <td>what would happen if the indian government sto...</td>\n",
              "      <td>0.667396</td>\n",
              "      <td>10</td>\n",
              "      <td>15</td>\n",
              "      <td>48</td>\n",
              "      <td>85</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>17</td>\n",
              "      <td>24</td>\n",
              "      <td>0.291667</td>\n",
              "      <td>65</td>\n",
              "      <td>73</td>\n",
              "      <td>63</td>\n",
              "      <td>86</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>how can i increase the speed of my internet co...</td>\n",
              "      <td>how can internet speed be increased by hacking...</td>\n",
              "      <td>0.499441</td>\n",
              "      <td>14</td>\n",
              "      <td>10</td>\n",
              "      <td>72</td>\n",
              "      <td>58</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>20</td>\n",
              "      <td>24</td>\n",
              "      <td>0.166667</td>\n",
              "      <td>54</td>\n",
              "      <td>53</td>\n",
              "      <td>66</td>\n",
              "      <td>66</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>7</td>\n",
              "      <td>8</td>\n",
              "      <td>why am i mentally very lonely how can i solve it</td>\n",
              "      <td>find the remainder when math 23 24 math is div...</td>\n",
              "      <td>0.165055</td>\n",
              "      <td>11</td>\n",
              "      <td>13</td>\n",
              "      <td>48</td>\n",
              "      <td>59</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>20</td>\n",
              "      <td>20</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>36</td>\n",
              "      <td>40</td>\n",
              "      <td>36</td>\n",
              "      <td>36</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>9</td>\n",
              "      <td>10</td>\n",
              "      <td>which one dissolve in water quikly sugar salt ...</td>\n",
              "      <td>which fish would survive in salt water</td>\n",
              "      <td>0.211917</td>\n",
              "      <td>13</td>\n",
              "      <td>7</td>\n",
              "      <td>73</td>\n",
              "      <td>38</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>16</td>\n",
              "      <td>20</td>\n",
              "      <td>0.200000</td>\n",
              "      <td>45</td>\n",
              "      <td>55</td>\n",
              "      <td>47</td>\n",
              "      <td>67</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>509787</th>\n",
              "      <td>1</td>\n",
              "      <td>235427</td>\n",
              "      <td>267265</td>\n",
              "      <td>should i join hcl tss leap program it is worth...</td>\n",
              "      <td>hcl tss best or not</td>\n",
              "      <td>0.472403</td>\n",
              "      <td>15</td>\n",
              "      <td>5</td>\n",
              "      <td>70</td>\n",
              "      <td>19</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>18</td>\n",
              "      <td>20</td>\n",
              "      <td>0.100000</td>\n",
              "      <td>36</td>\n",
              "      <td>63</td>\n",
              "      <td>29</td>\n",
              "      <td>54</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>509788</th>\n",
              "      <td>1</td>\n",
              "      <td>537762</td>\n",
              "      <td>132589</td>\n",
              "      <td>what is your favorite vodka drink and why</td>\n",
              "      <td>what is your favourite vodka</td>\n",
              "      <td>0.941730</td>\n",
              "      <td>8</td>\n",
              "      <td>5</td>\n",
              "      <td>41</td>\n",
              "      <td>28</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>9</td>\n",
              "      <td>13</td>\n",
              "      <td>0.307692</td>\n",
              "      <td>78</td>\n",
              "      <td>96</td>\n",
              "      <td>78</td>\n",
              "      <td>78</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>509789</th>\n",
              "      <td>1</td>\n",
              "      <td>537762</td>\n",
              "      <td>132589</td>\n",
              "      <td>what is your favorite vodka drink and why</td>\n",
              "      <td>what is your favourite vodka</td>\n",
              "      <td>0.941730</td>\n",
              "      <td>8</td>\n",
              "      <td>5</td>\n",
              "      <td>41</td>\n",
              "      <td>28</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>9</td>\n",
              "      <td>13</td>\n",
              "      <td>0.307692</td>\n",
              "      <td>78</td>\n",
              "      <td>96</td>\n",
              "      <td>78</td>\n",
              "      <td>78</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>509790</th>\n",
              "      <td>1</td>\n",
              "      <td>537894</td>\n",
              "      <td>187745</td>\n",
              "      <td>among bollywood stars which actor or actress d...</td>\n",
              "      <td>who are the over actors of bollywood</td>\n",
              "      <td>0.785160</td>\n",
              "      <td>12</td>\n",
              "      <td>7</td>\n",
              "      <td>70</td>\n",
              "      <td>36</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>17</td>\n",
              "      <td>19</td>\n",
              "      <td>0.105263</td>\n",
              "      <td>42</td>\n",
              "      <td>53</td>\n",
              "      <td>58</td>\n",
              "      <td>58</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>509791</th>\n",
              "      <td>1</td>\n",
              "      <td>537894</td>\n",
              "      <td>187745</td>\n",
              "      <td>among bollywood stars which actor or actress d...</td>\n",
              "      <td>who are the over actors of bollywood</td>\n",
              "      <td>0.785160</td>\n",
              "      <td>12</td>\n",
              "      <td>7</td>\n",
              "      <td>70</td>\n",
              "      <td>36</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>17</td>\n",
              "      <td>19</td>\n",
              "      <td>0.105263</td>\n",
              "      <td>42</td>\n",
              "      <td>53</td>\n",
              "      <td>58</td>\n",
              "      <td>58</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>509792 rows × 20 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        is_duplicate    qid1  ...  Token_Sort_Ratio Token_Set_Ratio\n",
              "0                  0       1  ...                93             100\n",
              "1                  0       3  ...                63              86\n",
              "2                  0       5  ...                66              66\n",
              "3                  0       7  ...                36              36\n",
              "4                  0       9  ...                47              67\n",
              "...              ...     ...  ...               ...             ...\n",
              "509787             1  235427  ...                29              54\n",
              "509788             1  537762  ...                78              78\n",
              "509789             1  537762  ...                78              78\n",
              "509790             1  537894  ...                58              58\n",
              "509791             1  537894  ...                58              58\n",
              "\n",
              "[509792 rows x 20 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fNrFO44vOe0f"
      },
      "source": [
        "# Model A"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "njnDh-cdWxwz"
      },
      "source": [
        "from keras import backend as K\n",
        "\n",
        "def recall_m(y_true, y_pred):\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
        "    recall = true_positives / (possible_positives + K.epsilon())\n",
        "    return recall\n",
        "\n",
        "def precision_m(y_true, y_pred):\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
        "    precision = true_positives / (predicted_positives + K.epsilon())\n",
        "    return precision\n",
        "\n",
        "def f1_m(y_true, y_pred):\n",
        "    precision = precision_m(y_true, y_pred)\n",
        "    recall = recall_m(y_true, y_pred)\n",
        "    return 2*((precision*recall)/(precision+recall+K.epsilon()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NSx-cNKvN6YX"
      },
      "source": [
        "def modelA(input_dim, output_dim, embedding_matrix, input_length):\n",
        "  seq1 = Input(shape=(input_length,))\n",
        "  seq2 = Input(shape=(input_length,))\n",
        "\n",
        "  front_model = tf.keras.Sequential()\n",
        "  front_model.add(Embedding(input_dim = input_dim,\n",
        "                        output_dim = output_dim,\n",
        "                        weights = [embedding_matrix],\n",
        "                        input_length = input_length,\n",
        "                        trainable=False))\n",
        "\n",
        "  front_model.add(LSTM(128, activation = 'tanh', return_sequences = True))\n",
        "  front_model.add(Dropout(0.2))\n",
        "  front_model.add(LSTM(128, activation = 'tanh', return_sequences = True))\n",
        "  front_model.add(Dropout(0.2))\n",
        "  front_model.add(LSTM(128))\n",
        "\n",
        "  output1 = front_model(seq1)\n",
        "  output2 = front_model(seq2)\n",
        "\n",
        "  merged = Multiply()([output1, output2])\n",
        "  merged = Flatten()(merged)\n",
        "  merged = Dense(128, activation = 'relu')(merged)\n",
        "  merged = BatchNormalization()(merged)\n",
        "  merged = Dropout(0.2)(merged)\n",
        "  merged = Dense(128, activation= 'relu')(merged)\n",
        "  merged = BatchNormalization()(merged)\n",
        "  merged = Dropout(0.2)(merged)\n",
        "  merged = Dense(1, activation = 'sigmoid')(merged)\n",
        "\n",
        "  newmodel = Model([seq1, seq2], merged)\n",
        "  newmodel.compile(optimizer = 'adam', loss = 'binary_crossentropy',\n",
        "                metrics= ['acc', f1_m, precision_m, recall_m])\n",
        "  \n",
        "  return newmodel\n",
        "\n",
        "#model A\n",
        "#sentence A -> common lstm -> output A\n",
        "#sentence B -> common lstm -> output B\n",
        "#multiply output A and output B and then put through some dense layers and finally a sigmoid function.\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K1J6Xdz8keVf"
      },
      "source": [
        "def modelB(input_dim, output_dim, embedding_matrix, input_length):\n",
        "  seq1 = Input(shape=(input_length,))\n",
        "  seq2 = Input(shape=(input_length,))\n",
        "\n",
        "  front_model = tf.keras.Sequential()\n",
        "  front_model.add(Embedding(input_dim = input_dim,\n",
        "                        output_dim = output_dim,\n",
        "                        weights = [embedding_matrix],\n",
        "                        input_length = input_length,\n",
        "                        trainable=False))\n",
        "\n",
        "  front_model.add(LSTM(128, activation = 'tanh', return_sequences = True))\n",
        "  front_model.add(Dropout(0.2))\n",
        "  front_model.add(LSTM(128, activation = 'tanh', return_sequences = True))\n",
        "  front_model.add(Dropout(0.2))\n",
        "  front_model.add(LSTM(128))\n",
        "\n",
        "  output1 = front_model(seq1)\n",
        "  output2 = front_model(seq2)\n",
        "\n",
        "  merged = concatenate([output1, output2])\n",
        "  merged = Flatten()(merged)\n",
        "  merged = Dense(128, activation = 'relu')(merged)\n",
        "  merged = BatchNormalization()(merged)\n",
        "  merged = Dropout(0.2)(merged)\n",
        "  merged = Dense(128, activation= 'relu')(merged)\n",
        "  merged = BatchNormalization()(merged)\n",
        "  merged = Dropout(0.2)(merged)\n",
        "  merged = Dense(1, activation = 'sigmoid')(merged)\n",
        "\n",
        "  newmodel = Model([seq1, seq2], merged)\n",
        "  newmodel.compile(optimizer = 'adam', loss = 'binary_crossentropy',\n",
        "                metrics= ['acc',f1_m,precision_m, recall_m])\n",
        "  \n",
        "  return newmodel\n",
        "\n",
        "\n",
        "#model B:\n",
        "#sentence A -> common lstm -> output A\n",
        "#sentence B -> common lstm -> output B\n",
        "#concatenate outputA and outputB and then put through some dense layers and finally a sigmoid function.\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ESKFokVSi4a"
      },
      "source": [
        "def modelC(input_dim, output_dim, embedding_matrix, input_length):\n",
        "  seq1 = Input(shape=(input_length,))\n",
        "  seq2 = Input(shape=(input_length,))\n",
        "\n",
        "  front_model = tf.keras.Sequential()\n",
        "  front_model.add(Embedding(input_dim = input_dim,\n",
        "                        output_dim = output_dim,\n",
        "                        weights = [embedding_matrix],\n",
        "                        input_length = input_length,\n",
        "                        trainable=False))\n",
        "\n",
        "  front_model.add(LSTM(128, activation = 'tanh', return_sequences = True))\n",
        "  front_model.add(Dropout(0.2))\n",
        "  front_model.add(LSTM(128))\n",
        "\n",
        "  output1 = front_model(seq1)\n",
        "  output2 = front_model(seq2)\n",
        "\n",
        "  merged = Multiply()([output1, output2])\n",
        "  merged = Flatten()(merged)\n",
        "  merged = Dense(128, activation = 'relu')(merged)\n",
        "  merged = BatchNormalization()(merged)\n",
        "  merged = Dropout(0.2)(merged)\n",
        "  merged = Dense(1, activation = 'sigmoid')(merged)\n",
        "\n",
        "  newmodel = Model([seq1, seq2], merged)\n",
        "  newmodel.compile(optimizer = 'adam', loss = 'binary_crossentropy',\n",
        "                metrics= ['acc', f1_m, precision_m, recall_m])\n",
        "  \n",
        "  return newmodel\n",
        "\n",
        "#model A\n",
        "#sentence A -> common lstm -> output A\n",
        "#sentence B -> common lstm -> output B\n",
        "#multiply output A and output B and then put through some dense layers and finally a sigmoid function.\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4n8-tHjXWcAA"
      },
      "source": [
        "def modelD(input_dim, output_dim, embedding_matrix, input_length):\n",
        "  seq1 = Input(shape=(input_length,))\n",
        "  seq2 = Input(shape=(input_length,))\n",
        "\n",
        "  front_model = tf.keras.Sequential()\n",
        "  front_model.add(Embedding(input_dim = input_dim,\n",
        "                        output_dim = output_dim,\n",
        "                        weights = [embedding_matrix],\n",
        "                        input_length = input_length,\n",
        "                        trainable=False))\n",
        "\n",
        "  front_model.add(LSTM(128, activation = 'tanh', return_sequences = True))\n",
        "  front_model.add(Dropout(0.2))\n",
        "  front_model.add(LSTM(128))\n",
        "\n",
        "  output1 = front_model(seq1)\n",
        "  output2 = front_model(seq2)\n",
        "\n",
        "  def exponent_neg_manhattan_distance(left, right):\n",
        "    return K.exp(-K.sum(K.abs(left-right), axis=1, keepdims=True))\n",
        "\n",
        "  # Calculates the distance as defined by the MaLSTM model\n",
        "  malstm_distance = Lambda(function=lambda x: exponent_neg_manhattan_distance(x[0], x[1]),output_shape=lambda x: (x[0][0], 1))([output1, output2])\n",
        "\n",
        "  # Pack it all up into a model\n",
        "  malstm = Model([seq1, seq2], [malstm_distance])\n",
        "\n",
        "  malstm.compile( optimizer='adam', loss='mean_squared_error', metrics=['acc',f1_m,precision_m, recall_m])\n",
        "\n",
        "  return malstm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sTnxHvwrOiyC"
      },
      "source": [
        "ideas:\n",
        "change model structure -> add/reduce more lstm layer, add/reduce more dense layers\n",
        "\n",
        "change word embedding used -> 100 dimension glove embeddings\n",
        "\n",
        "\n",
        "change loss and metrics to accuracy and binary_crossentropy\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B4-w2eHr5nEJ"
      },
      "source": [
        "input_length = 36\n",
        "output_dim = 50\n",
        "MAX_NB_WORDS = 200000"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pZQgi8DF5PMs"
      },
      "source": [
        "def preprocessing(question1_train_list, question2_train_list, Y_train_list, question1_test_list, question2_test_list, Y_test_list):\n",
        "  tokenizer = Tokenizer(num_words = MAX_NB_WORDS)\n",
        "  sentence_list = question1_train_list + question2_train_list\n",
        "  tokenizer.fit_on_texts(sentence_list)\n",
        "\n",
        "  X_train_q1 = tokenizer.texts_to_sequences(question1_train_list)\n",
        "  X_train_q1 = pad_sequences(X_train_q1, maxlen = input_length, padding='post')\n",
        "\n",
        "  X_train_q2 = tokenizer.texts_to_sequences(question2_train_list)\n",
        "  X_train_q2 = pad_sequences(X_train_q2, maxlen = input_length, padding='post')\n",
        "\n",
        "  word_index = tokenizer.word_index\n",
        "  input_dim = len(word_index)+1\n",
        "  embedding_matrix = np.random.random((input_dim, output_dim))\n",
        "  for word, i in word_index.items():\n",
        "      embedding_vector = embeddings_dict.get(word)\n",
        "      if embedding_vector is not None:\n",
        "          embedding_matrix[i] = embedding_vector\n",
        "  \n",
        "  Y_train = np.asarray(Y_train_list)\n",
        "  Y_test = np.asarray(Y_test_list)\n",
        "  \n",
        "  X_test_q1 = tokenizer.texts_to_sequences(question1_test_list)\n",
        "  X_test_q1 = pad_sequences(X_test_q1, maxlen = input_length, padding='post')\n",
        "\n",
        "  X_test_q2 = tokenizer.texts_to_sequences(question2_test_list)\n",
        "  X_test_q2 = pad_sequences(X_test_q2, maxlen = input_length, padding='post')\n",
        "\n",
        "  return input_dim, embedding_matrix, X_train_q1, X_train_q2, Y_train, X_test_q1, X_test_q2, Y_test, tokenizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VlUSOZEQPX-W"
      },
      "source": [
        "def train_model(modeltype, batch_size, epochs, input_dim, embedding_matrix, X_train_q1, X_train_q2, Y_train, X_test_q1, X_test_q2, Y_test):\n",
        "  if modeltype == 'A':\n",
        "    new_model = modelA(input_dim, output_dim, embedding_matrix, input_length)\n",
        "    model_string = '/content/gdrive/MyDrive/QuoraQuestions/modelA/modelA-{epoch:03d}-{acc:03f}-{val_acc:03f}.h5'\n",
        "  elif modeltype == 'B':\n",
        "    new_model = modelB(input_dim, output_dim, embedding_matrix, input_length)\n",
        "    model_string = '/content/gdrive/MyDrive/QuoraQuestions/modelB/modelB-{epoch:03d}-{acc:03f}-{val_acc:03f}.h5'\n",
        "  elif modeltype == 'C':\n",
        "    new_model = modelC(input_dim, output_dim, embedding_matrix, input_length)\n",
        "    model_string = '/content/gdrive/MyDrive/QuoraQuestions/modelC/modelC-{epoch:03d}-{acc:03f}-{val_acc:03f}.h5'\n",
        "  elif modeltype == 'D':\n",
        "    new_model = modelD(input_dim, output_dim, embedding_matrix, input_length)\n",
        "    model_string = '/content/gdrive/MyDrive/QuoraQuestions/modelD/modelD-{epoch:03d}-{acc:03f}-{val_acc:03f}.h5'\n",
        "  \n",
        "  print(new_model.summary())\n",
        "\n",
        "  checkpoint_save = ModelCheckpoint(model_string, verbose=0, monitor='val_acc',save_best_only=True, mode='max')  \n",
        "\n",
        "  history = new_model.fit([X_train_q1, X_train_q2], Y_train,\n",
        "                          validation_data=([X_test_q1, X_test_q2], Y_test), \n",
        "                          callbacks=[checkpoint_save],\n",
        "                          batch_size = batch_size, epochs = epochs, shuffle=True)\n",
        "\n",
        "  results = new_model.evaluate([X_test_q1, X_test_q2], Y_test, verbose=0)\n",
        "\n",
        "  return new_model, history, results"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2iXndaQg6Vf-"
      },
      "source": [
        "def fullpipeline(question1_train_list, question2_train_list, Y_train_list, question1_test_list, question2_test_list, Y_test_list, modeltype, batch_size, epochs):\n",
        "  input_dim, embedding_matrix, X_train_q1, X_train_q2, Y_train, X_test_q1, X_test_q2, Y_test, my_tokenizer = preprocessing(question1_train_list, question2_train_list, Y_train_list, question1_test_list, question2_test_list, Y_test_list)\n",
        "  new_model, history, results = train_model(modeltype, batch_size, epochs, input_dim, embedding_matrix, X_train_q1, X_train_q2, Y_train, X_test_q1, X_test_q2, Y_test)\n",
        "  return new_model, history, results"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dbyGYBfOOht_"
      },
      "source": [
        "# Load data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1mSOv08jU-Qe"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "batch_size = 2048\n",
        "epochs = 35"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MtDegLXmS61j"
      },
      "source": [
        "X_train_q1 = train_df['question1_cleaned'].astype(str).tolist()\n",
        "X_train_q2 = train_df['question2_cleaned'].astype(str).tolist()\n",
        "Y_train = train_df['is_duplicate'].astype(int).tolist()\n",
        "\n",
        "q1_train, q1_val, q2_train, q2_val, y_train, y_val = train_test_split(X_train_q1, X_train_q2, Y_train, test_size = 0.10, random_state=7)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sfT83if9Zkd6",
        "outputId": "8c3715a1-9fa0-4ce5-ae24-819ca0c24833"
      },
      "source": [
        "print(\"number of training samples: \", len(q1_train))\n",
        "print(\"number of non-duplicate samples: \", y_train.count(0))\n",
        "print(\"number of duplicate samples: \", y_train.count(1))\n",
        "\n",
        "\n",
        "print(\"number of training samples: \", len(q1_val))\n",
        "print(\"number of non-duplicate samples: \", y_val.count(0))\n",
        "print(\"number of duplicate samples: \", y_val.count(1))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "number of training samples:  458812\n",
            "number of non-duplicate samples:  229318\n",
            "number of duplicate samples:  229494\n",
            "number of training samples:  50980\n",
            "number of non-duplicate samples:  25578\n",
            "number of duplicate samples:  25402\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5DYTC0KvciyH"
      },
      "source": [
        "# Preprocess data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tkYUmNEESDc0"
      },
      "source": [
        "input_dim, embedding_matrix, X_train_q1, X_train_q2, Y_train, X_test_q1, X_test_q2, Y_test, my_tokenizer = preprocessing(q1_train, q2_train, y_train, q1_val, q2_val, y_val)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fwmTYdHIaR2b"
      },
      "source": [
        "import pickle\n",
        "# saving tokenizer\n",
        "with open('/content/gdrive/MyDrive/QuoraQuestions/mytokenizer.pickle', 'wb') as handle:\n",
        "    pickle.dump(my_tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rv4kpWYLk7Sc"
      },
      "source": [
        "# training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TbCbboJpVAUu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6ca90501-4029-4fa0-8df1-508c4a15a3e0"
      },
      "source": [
        "mymodelA, train_historyA, eval_resultsA = train_model('A', batch_size, epochs, input_dim, embedding_matrix, X_train_q1, X_train_q2, Y_train, X_test_q1, X_test_q2, Y_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, 36)]         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_2 (InputLayer)            [(None, 36)]         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "sequential (Sequential)         (None, 128)          4468616     input_1[0][0]                    \n",
            "                                                                 input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "multiply (Multiply)             (None, 128)          0           sequential[0][0]                 \n",
            "                                                                 sequential[1][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "flatten (Flatten)               (None, 128)          0           multiply[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, 128)          16512       flatten[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization (BatchNorma (None, 128)          512         dense[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "dropout_2 (Dropout)             (None, 128)          0           batch_normalization[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 128)          16512       dropout_2[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1 (BatchNor (None, 128)          512         dense_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_3 (Dropout)             (None, 128)          0           batch_normalization_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "dense_2 (Dense)                 (None, 1)            129         dropout_3[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 4,502,793\n",
            "Trainable params: 388,481\n",
            "Non-trainable params: 4,114,312\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Epoch 1/35\n",
            "225/225 [==============================] - 92s 360ms/step - loss: 0.5760 - acc: 0.6915 - f1_m: 0.6843 - precision_m: 0.7092 - recall_m: 0.6634 - val_loss: 0.6241 - val_acc: 0.6028 - val_f1_m: 0.7067 - val_precision_m: 0.5593 - val_recall_m: 0.9601\n",
            "Epoch 2/35\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
            "  category=CustomMaskWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "225/225 [==============================] - 78s 347ms/step - loss: 0.4905 - acc: 0.7556 - f1_m: 0.7447 - precision_m: 0.7792 - recall_m: 0.7141 - val_loss: 0.6349 - val_acc: 0.5846 - val_f1_m: 0.3112 - val_precision_m: 0.8947 - val_recall_m: 0.1885\n",
            "Epoch 3/35\n",
            "225/225 [==============================] - 78s 347ms/step - loss: 0.4677 - acc: 0.7684 - f1_m: 0.7590 - precision_m: 0.7916 - recall_m: 0.7300 - val_loss: 0.4840 - val_acc: 0.7490 - val_f1_m: 0.7682 - val_precision_m: 0.7114 - val_recall_m: 0.8350\n",
            "Epoch 4/35\n",
            "225/225 [==============================] - 78s 347ms/step - loss: 0.4474 - acc: 0.7794 - f1_m: 0.7731 - precision_m: 0.7954 - recall_m: 0.7529 - val_loss: 1.0979 - val_acc: 0.6173 - val_f1_m: 0.7129 - val_precision_m: 0.5694 - val_recall_m: 0.9537\n",
            "Epoch 5/35\n",
            "225/225 [==============================] - 78s 347ms/step - loss: 0.4402 - acc: 0.7836 - f1_m: 0.7787 - precision_m: 0.7984 - recall_m: 0.7606 - val_loss: 0.4398 - val_acc: 0.7833 - val_f1_m: 0.7708 - val_precision_m: 0.8148 - val_recall_m: 0.7314\n",
            "Epoch 6/35\n",
            "225/225 [==============================] - 78s 346ms/step - loss: 0.4258 - acc: 0.7918 - f1_m: 0.7885 - precision_m: 0.8014 - recall_m: 0.7768 - val_loss: 0.5108 - val_acc: 0.7345 - val_f1_m: 0.6920 - val_precision_m: 0.8196 - val_recall_m: 0.5988\n",
            "Epoch 7/35\n",
            "225/225 [==============================] - 78s 345ms/step - loss: 0.4178 - acc: 0.7963 - f1_m: 0.7939 - precision_m: 0.8039 - recall_m: 0.7847 - val_loss: 0.4663 - val_acc: 0.7731 - val_f1_m: 0.7930 - val_precision_m: 0.7266 - val_recall_m: 0.8729\n",
            "Epoch 8/35\n",
            "225/225 [==============================] - 78s 345ms/step - loss: 0.4072 - acc: 0.8029 - f1_m: 0.8013 - precision_m: 0.8078 - recall_m: 0.7956 - val_loss: 0.4553 - val_acc: 0.7856 - val_f1_m: 0.7997 - val_precision_m: 0.7476 - val_recall_m: 0.8597\n",
            "Epoch 9/35\n",
            "225/225 [==============================] - 78s 348ms/step - loss: 0.3966 - acc: 0.8095 - f1_m: 0.8084 - precision_m: 0.8124 - recall_m: 0.8051 - val_loss: 0.5061 - val_acc: 0.7628 - val_f1_m: 0.7939 - val_precision_m: 0.7002 - val_recall_m: 0.9169\n",
            "Epoch 10/35\n",
            "225/225 [==============================] - 79s 350ms/step - loss: 0.3884 - acc: 0.8141 - f1_m: 0.8135 - precision_m: 0.8172 - recall_m: 0.8102 - val_loss: 0.3979 - val_acc: 0.8079 - val_f1_m: 0.8107 - val_precision_m: 0.7964 - val_recall_m: 0.8256\n",
            "Epoch 11/35\n",
            "225/225 [==============================] - 79s 350ms/step - loss: 0.3783 - acc: 0.8206 - f1_m: 0.8208 - precision_m: 0.8208 - recall_m: 0.8212 - val_loss: 0.3997 - val_acc: 0.8115 - val_f1_m: 0.8040 - val_precision_m: 0.8338 - val_recall_m: 0.7762\n",
            "Epoch 12/35\n",
            "225/225 [==============================] - 79s 352ms/step - loss: 0.3702 - acc: 0.8254 - f1_m: 0.8250 - precision_m: 0.8239 - recall_m: 0.8267 - val_loss: 0.3882 - val_acc: 0.8141 - val_f1_m: 0.8185 - val_precision_m: 0.7966 - val_recall_m: 0.8416\n",
            "Epoch 13/35\n",
            "225/225 [==============================] - 79s 350ms/step - loss: 0.3617 - acc: 0.8300 - f1_m: 0.8305 - precision_m: 0.8284 - recall_m: 0.8330 - val_loss: 0.3819 - val_acc: 0.8197 - val_f1_m: 0.8199 - val_precision_m: 0.8160 - val_recall_m: 0.8241\n",
            "Epoch 14/35\n",
            "225/225 [==============================] - 78s 349ms/step - loss: 0.3534 - acc: 0.8346 - f1_m: 0.8354 - precision_m: 0.8323 - recall_m: 0.8390 - val_loss: 0.3695 - val_acc: 0.8257 - val_f1_m: 0.8276 - val_precision_m: 0.8153 - val_recall_m: 0.8405\n",
            "Epoch 15/35\n",
            "225/225 [==============================] - 78s 348ms/step - loss: 0.3432 - acc: 0.8406 - f1_m: 0.8413 - precision_m: 0.8376 - recall_m: 0.8455 - val_loss: 0.3692 - val_acc: 0.8271 - val_f1_m: 0.8269 - val_precision_m: 0.8249 - val_recall_m: 0.8290\n",
            "Epoch 16/35\n",
            "225/225 [==============================] - 79s 349ms/step - loss: 0.3367 - acc: 0.8443 - f1_m: 0.8450 - precision_m: 0.8403 - recall_m: 0.8502 - val_loss: 0.3711 - val_acc: 0.8284 - val_f1_m: 0.8325 - val_precision_m: 0.8102 - val_recall_m: 0.8561\n",
            "Epoch 17/35\n",
            "225/225 [==============================] - 78s 347ms/step - loss: 0.3299 - acc: 0.8480 - f1_m: 0.8492 - precision_m: 0.8433 - recall_m: 0.8555 - val_loss: 0.3959 - val_acc: 0.8190 - val_f1_m: 0.8313 - val_precision_m: 0.7758 - val_recall_m: 0.8954\n",
            "Epoch 18/35\n",
            "225/225 [==============================] - 78s 347ms/step - loss: 0.3232 - acc: 0.8521 - f1_m: 0.8529 - precision_m: 0.8465 - recall_m: 0.8599 - val_loss: 0.3793 - val_acc: 0.8183 - val_f1_m: 0.8272 - val_precision_m: 0.7858 - val_recall_m: 0.8732\n",
            "Epoch 19/35\n",
            "225/225 [==============================] - 78s 348ms/step - loss: 0.3309 - acc: 0.8475 - f1_m: 0.8482 - precision_m: 0.8435 - recall_m: 0.8534 - val_loss: 0.3785 - val_acc: 0.8283 - val_f1_m: 0.8386 - val_precision_m: 0.7883 - val_recall_m: 0.8958\n",
            "Epoch 20/35\n",
            "225/225 [==============================] - 78s 348ms/step - loss: 0.3089 - acc: 0.8598 - f1_m: 0.8615 - precision_m: 0.8544 - recall_m: 0.8691 - val_loss: 0.3523 - val_acc: 0.8379 - val_f1_m: 0.8451 - val_precision_m: 0.8063 - val_recall_m: 0.8879\n",
            "Epoch 21/35\n",
            "225/225 [==============================] - 79s 350ms/step - loss: 0.2992 - acc: 0.8651 - f1_m: 0.8665 - precision_m: 0.8580 - recall_m: 0.8755 - val_loss: 0.3695 - val_acc: 0.8367 - val_f1_m: 0.8429 - val_precision_m: 0.8095 - val_recall_m: 0.8792\n",
            "Epoch 22/35\n",
            "225/225 [==============================] - 78s 349ms/step - loss: 0.2924 - acc: 0.8678 - f1_m: 0.8688 - precision_m: 0.8612 - recall_m: 0.8768 - val_loss: 0.3476 - val_acc: 0.8438 - val_f1_m: 0.8436 - val_precision_m: 0.8416 - val_recall_m: 0.8457\n",
            "Epoch 23/35\n",
            "225/225 [==============================] - 78s 348ms/step - loss: 0.2853 - acc: 0.8722 - f1_m: 0.8735 - precision_m: 0.8660 - recall_m: 0.8814 - val_loss: 0.3398 - val_acc: 0.8479 - val_f1_m: 0.8502 - val_precision_m: 0.8345 - val_recall_m: 0.8665\n",
            "Epoch 24/35\n",
            "225/225 [==============================] - 79s 350ms/step - loss: 0.2780 - acc: 0.8758 - f1_m: 0.8769 - precision_m: 0.8685 - recall_m: 0.8857 - val_loss: 0.3429 - val_acc: 0.8475 - val_f1_m: 0.8497 - val_precision_m: 0.8345 - val_recall_m: 0.8655\n",
            "Epoch 25/35\n",
            "225/225 [==============================] - 79s 350ms/step - loss: 0.2724 - acc: 0.8790 - f1_m: 0.8805 - precision_m: 0.8726 - recall_m: 0.8890 - val_loss: 0.3556 - val_acc: 0.8391 - val_f1_m: 0.8485 - val_precision_m: 0.7990 - val_recall_m: 0.9045\n",
            "Epoch 26/35\n",
            "225/225 [==============================] - 78s 348ms/step - loss: 0.2639 - acc: 0.8835 - f1_m: 0.8848 - precision_m: 0.8765 - recall_m: 0.8935 - val_loss: 0.3390 - val_acc: 0.8511 - val_f1_m: 0.8524 - val_precision_m: 0.8418 - val_recall_m: 0.8633\n",
            "Epoch 27/35\n",
            "225/225 [==============================] - 78s 348ms/step - loss: 0.2575 - acc: 0.8867 - f1_m: 0.8878 - precision_m: 0.8786 - recall_m: 0.8976 - val_loss: 0.3445 - val_acc: 0.8499 - val_f1_m: 0.8554 - val_precision_m: 0.8228 - val_recall_m: 0.8907\n",
            "Epoch 28/35\n",
            "225/225 [==============================] - 79s 350ms/step - loss: 0.2519 - acc: 0.8898 - f1_m: 0.8908 - precision_m: 0.8811 - recall_m: 0.9011 - val_loss: 0.3378 - val_acc: 0.8554 - val_f1_m: 0.8534 - val_precision_m: 0.8616 - val_recall_m: 0.8455\n",
            "Epoch 29/35\n",
            "225/225 [==============================] - 78s 348ms/step - loss: 0.2458 - acc: 0.8929 - f1_m: 0.8938 - precision_m: 0.8862 - recall_m: 0.9018 - val_loss: 0.3412 - val_acc: 0.8549 - val_f1_m: 0.8528 - val_precision_m: 0.8620 - val_recall_m: 0.8438\n",
            "Epoch 30/35\n",
            "225/225 [==============================] - 79s 349ms/step - loss: 0.2393 - acc: 0.8961 - f1_m: 0.8968 - precision_m: 0.8877 - recall_m: 0.9064 - val_loss: 0.4098 - val_acc: 0.8408 - val_f1_m: 0.8500 - val_precision_m: 0.8009 - val_recall_m: 0.9055\n",
            "Epoch 31/35\n",
            "225/225 [==============================] - 78s 348ms/step - loss: 0.2329 - acc: 0.8995 - f1_m: 0.9004 - precision_m: 0.8924 - recall_m: 0.9089 - val_loss: 0.3370 - val_acc: 0.8561 - val_f1_m: 0.8582 - val_precision_m: 0.8423 - val_recall_m: 0.8749\n",
            "Epoch 32/35\n",
            "225/225 [==============================] - 78s 349ms/step - loss: 0.2290 - acc: 0.9011 - f1_m: 0.9021 - precision_m: 0.8938 - recall_m: 0.9108 - val_loss: 0.3462 - val_acc: 0.8557 - val_f1_m: 0.8573 - val_precision_m: 0.8445 - val_recall_m: 0.8706\n",
            "Epoch 33/35\n",
            "225/225 [==============================] - 79s 351ms/step - loss: 0.2306 - acc: 0.9012 - f1_m: 0.9022 - precision_m: 0.8945 - recall_m: 0.9103 - val_loss: 0.3948 - val_acc: 0.8485 - val_f1_m: 0.8552 - val_precision_m: 0.8165 - val_recall_m: 0.8978\n",
            "Epoch 34/35\n",
            "225/225 [==============================] - 78s 349ms/step - loss: 0.2219 - acc: 0.9050 - f1_m: 0.9059 - precision_m: 0.8977 - recall_m: 0.9146 - val_loss: 0.3591 - val_acc: 0.8551 - val_f1_m: 0.8610 - val_precision_m: 0.8247 - val_recall_m: 0.9008\n",
            "Epoch 35/35\n",
            "225/225 [==============================] - 78s 348ms/step - loss: 0.2122 - acc: 0.9098 - f1_m: 0.9107 - precision_m: 0.9021 - recall_m: 0.9198 - val_loss: 0.3646 - val_acc: 0.8542 - val_f1_m: 0.8607 - val_precision_m: 0.8213 - val_recall_m: 0.9042\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D9fYhlczHNkY",
        "outputId": "d224fe29-96bc-41e1-916e-884a5fad9d9d"
      },
      "source": [
        "mymodelA.save('/content/gdrive/MyDrive/QuoraQuestions/modelA_last.h5')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
            "  category=CustomMaskWarning)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "efOiujDFkAz0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5b599b87-2108-4aef-8da5-029a47d81934"
      },
      "source": [
        "mymodelB, train_historyB, eval_resultsB = train_model('B', batch_size, epochs, input_dim, embedding_matrix, X_train_q1, X_train_q2, Y_train, X_test_q1, X_test_q2, Y_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_3 (InputLayer)            [(None, 36)]         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_4 (InputLayer)            [(None, 36)]         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "sequential_1 (Sequential)       (None, 128)          4468616     input_3[0][0]                    \n",
            "                                                                 input_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "concatenate (Concatenate)       (None, 256)          0           sequential_1[0][0]               \n",
            "                                                                 sequential_1[1][0]               \n",
            "__________________________________________________________________________________________________\n",
            "flatten_1 (Flatten)             (None, 256)          0           concatenate[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dense_3 (Dense)                 (None, 128)          32896       flatten_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_2 (BatchNor (None, 128)          512         dense_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_6 (Dropout)             (None, 128)          0           batch_normalization_2[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "dense_4 (Dense)                 (None, 128)          16512       dropout_6[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_3 (BatchNor (None, 128)          512         dense_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_7 (Dropout)             (None, 128)          0           batch_normalization_3[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "dense_5 (Dense)                 (None, 1)            129         dropout_7[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 4,519,177\n",
            "Trainable params: 404,865\n",
            "Non-trainable params: 4,114,312\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Epoch 1/35\n",
            "225/225 [==============================] - 91s 361ms/step - loss: 0.5688 - acc: 0.6989 - f1_m: 0.6922 - precision_m: 0.7174 - recall_m: 0.6711 - val_loss: 0.5296 - val_acc: 0.7291 - val_f1_m: 0.7414 - val_precision_m: 0.7069 - val_recall_m: 0.7796\n",
            "Epoch 2/35\n",
            "225/225 [==============================] - 78s 348ms/step - loss: 0.4865 - acc: 0.7593 - f1_m: 0.7467 - precision_m: 0.7884 - recall_m: 0.7100 - val_loss: 0.6058 - val_acc: 0.6347 - val_f1_m: 0.4299 - val_precision_m: 0.9658 - val_recall_m: 0.2766\n",
            "Epoch 3/35\n",
            "225/225 [==============================] - 78s 349ms/step - loss: 0.4630 - acc: 0.7732 - f1_m: 0.7627 - precision_m: 0.7998 - recall_m: 0.7296 - val_loss: 0.5634 - val_acc: 0.7475 - val_f1_m: 0.7632 - val_precision_m: 0.7162 - val_recall_m: 0.8171\n",
            "Epoch 4/35\n",
            "225/225 [==============================] - 78s 349ms/step - loss: 0.4480 - acc: 0.7812 - f1_m: 0.7735 - precision_m: 0.8024 - recall_m: 0.7473 - val_loss: 0.4644 - val_acc: 0.7674 - val_f1_m: 0.7740 - val_precision_m: 0.7501 - val_recall_m: 0.7996\n",
            "Epoch 5/35\n",
            "225/225 [==============================] - 78s 348ms/step - loss: 0.4361 - acc: 0.7873 - f1_m: 0.7803 - precision_m: 0.8069 - recall_m: 0.7560 - val_loss: 0.5011 - val_acc: 0.7433 - val_f1_m: 0.6805 - val_precision_m: 0.8960 - val_recall_m: 0.5487\n",
            "Epoch 6/35\n",
            "225/225 [==============================] - 78s 348ms/step - loss: 0.4266 - acc: 0.7927 - f1_m: 0.7872 - precision_m: 0.8089 - recall_m: 0.7675 - val_loss: 0.5618 - val_acc: 0.6556 - val_f1_m: 0.4806 - val_precision_m: 0.9668 - val_recall_m: 0.3199\n",
            "Epoch 7/35\n",
            "225/225 [==============================] - 78s 348ms/step - loss: 0.4141 - acc: 0.8003 - f1_m: 0.7963 - precision_m: 0.8123 - recall_m: 0.7815 - val_loss: 0.4910 - val_acc: 0.7698 - val_f1_m: 0.7873 - val_precision_m: 0.7294 - val_recall_m: 0.8554\n",
            "Epoch 8/35\n",
            "225/225 [==============================] - 78s 349ms/step - loss: 0.4044 - acc: 0.8061 - f1_m: 0.8036 - precision_m: 0.8152 - recall_m: 0.7929 - val_loss: 0.4560 - val_acc: 0.7735 - val_f1_m: 0.7428 - val_precision_m: 0.8551 - val_recall_m: 0.6568\n",
            "Epoch 9/35\n",
            "225/225 [==============================] - 78s 348ms/step - loss: 0.3986 - acc: 0.8092 - f1_m: 0.8067 - precision_m: 0.8160 - recall_m: 0.7981 - val_loss: 0.4237 - val_acc: 0.7954 - val_f1_m: 0.7863 - val_precision_m: 0.8196 - val_recall_m: 0.7557\n",
            "Epoch 10/35\n",
            "225/225 [==============================] - 79s 349ms/step - loss: 0.3860 - acc: 0.8167 - f1_m: 0.8153 - precision_m: 0.8224 - recall_m: 0.8087 - val_loss: 0.4126 - val_acc: 0.7998 - val_f1_m: 0.8128 - val_precision_m: 0.7607 - val_recall_m: 0.8727\n",
            "Epoch 11/35\n",
            "225/225 [==============================] - 78s 347ms/step - loss: 0.3774 - acc: 0.8218 - f1_m: 0.8215 - precision_m: 0.8243 - recall_m: 0.8191 - val_loss: 0.4223 - val_acc: 0.7981 - val_f1_m: 0.7724 - val_precision_m: 0.8807 - val_recall_m: 0.6880\n",
            "Epoch 12/35\n",
            "225/225 [==============================] - 78s 348ms/step - loss: 0.3703 - acc: 0.8256 - f1_m: 0.8251 - precision_m: 0.8266 - recall_m: 0.8239 - val_loss: 0.4060 - val_acc: 0.8056 - val_f1_m: 0.7894 - val_precision_m: 0.8576 - val_recall_m: 0.7314\n",
            "Epoch 13/35\n",
            "225/225 [==============================] - 79s 350ms/step - loss: 0.3645 - acc: 0.8291 - f1_m: 0.8293 - precision_m: 0.8288 - recall_m: 0.8302 - val_loss: 0.3883 - val_acc: 0.8177 - val_f1_m: 0.8185 - val_precision_m: 0.8119 - val_recall_m: 0.8253\n",
            "Epoch 14/35\n",
            "225/225 [==============================] - 78s 349ms/step - loss: 0.3570 - acc: 0.8335 - f1_m: 0.8338 - precision_m: 0.8327 - recall_m: 0.8354 - val_loss: 0.3892 - val_acc: 0.8162 - val_f1_m: 0.8110 - val_precision_m: 0.8314 - val_recall_m: 0.7917\n",
            "Epoch 15/35\n",
            "225/225 [==============================] - 78s 349ms/step - loss: 0.3515 - acc: 0.8365 - f1_m: 0.8371 - precision_m: 0.8339 - recall_m: 0.8407 - val_loss: 0.4384 - val_acc: 0.7878 - val_f1_m: 0.7667 - val_precision_m: 0.8477 - val_recall_m: 0.7000\n",
            "Epoch 16/35\n",
            "225/225 [==============================] - 79s 350ms/step - loss: 0.3472 - acc: 0.8387 - f1_m: 0.8391 - precision_m: 0.8361 - recall_m: 0.8426 - val_loss: 0.3869 - val_acc: 0.8200 - val_f1_m: 0.8085 - val_precision_m: 0.8601 - val_recall_m: 0.7629\n",
            "Epoch 17/35\n",
            "225/225 [==============================] - 78s 347ms/step - loss: 0.3381 - acc: 0.8431 - f1_m: 0.8442 - precision_m: 0.8399 - recall_m: 0.8489 - val_loss: 0.3870 - val_acc: 0.8218 - val_f1_m: 0.8137 - val_precision_m: 0.8484 - val_recall_m: 0.7819\n",
            "Epoch 18/35\n",
            "225/225 [==============================] - 78s 348ms/step - loss: 0.3325 - acc: 0.8466 - f1_m: 0.8471 - precision_m: 0.8424 - recall_m: 0.8523 - val_loss: 0.4172 - val_acc: 0.8001 - val_f1_m: 0.7823 - val_precision_m: 0.8550 - val_recall_m: 0.7212\n",
            "Epoch 19/35\n",
            "225/225 [==============================] - 78s 347ms/step - loss: 0.3282 - acc: 0.8490 - f1_m: 0.8498 - precision_m: 0.8452 - recall_m: 0.8548 - val_loss: 0.4111 - val_acc: 0.8212 - val_f1_m: 0.8297 - val_precision_m: 0.7895 - val_recall_m: 0.8743\n",
            "Epoch 20/35\n",
            "225/225 [==============================] - 78s 348ms/step - loss: 0.3199 - acc: 0.8539 - f1_m: 0.8552 - precision_m: 0.8488 - recall_m: 0.8621 - val_loss: 0.3838 - val_acc: 0.8243 - val_f1_m: 0.8346 - val_precision_m: 0.7856 - val_recall_m: 0.8902\n",
            "Epoch 21/35\n",
            "225/225 [==============================] - 78s 347ms/step - loss: 0.3132 - acc: 0.8570 - f1_m: 0.8582 - precision_m: 0.8507 - recall_m: 0.8662 - val_loss: 0.3688 - val_acc: 0.8367 - val_f1_m: 0.8380 - val_precision_m: 0.8280 - val_recall_m: 0.8484\n",
            "Epoch 22/35\n",
            "225/225 [==============================] - 78s 347ms/step - loss: 0.3074 - acc: 0.8604 - f1_m: 0.8614 - precision_m: 0.8544 - recall_m: 0.8687 - val_loss: 0.3531 - val_acc: 0.8395 - val_f1_m: 0.8360 - val_precision_m: 0.8513 - val_recall_m: 0.8212\n",
            "Epoch 23/35\n",
            "225/225 [==============================] - 79s 349ms/step - loss: 0.3026 - acc: 0.8625 - f1_m: 0.8637 - precision_m: 0.8559 - recall_m: 0.8718 - val_loss: 0.3657 - val_acc: 0.8328 - val_f1_m: 0.8285 - val_precision_m: 0.8469 - val_recall_m: 0.8110\n",
            "Epoch 24/35\n",
            "225/225 [==============================] - 78s 347ms/step - loss: 0.2973 - acc: 0.8652 - f1_m: 0.8663 - precision_m: 0.8584 - recall_m: 0.8746 - val_loss: 0.3662 - val_acc: 0.8329 - val_f1_m: 0.8416 - val_precision_m: 0.7972 - val_recall_m: 0.8914\n",
            "Epoch 25/35\n",
            "225/225 [==============================] - 78s 349ms/step - loss: 0.2895 - acc: 0.8694 - f1_m: 0.8705 - precision_m: 0.8630 - recall_m: 0.8785 - val_loss: 0.3618 - val_acc: 0.8392 - val_f1_m: 0.8428 - val_precision_m: 0.8212 - val_recall_m: 0.8657\n",
            "Epoch 26/35\n",
            "225/225 [==============================] - 79s 349ms/step - loss: 0.2856 - acc: 0.8717 - f1_m: 0.8734 - precision_m: 0.8641 - recall_m: 0.8831 - val_loss: 0.3731 - val_acc: 0.8392 - val_f1_m: 0.8432 - val_precision_m: 0.8196 - val_recall_m: 0.8683\n",
            "Epoch 27/35\n",
            "225/225 [==============================] - 79s 350ms/step - loss: 0.2784 - acc: 0.8753 - f1_m: 0.8764 - precision_m: 0.8675 - recall_m: 0.8856 - val_loss: 0.3875 - val_acc: 0.8272 - val_f1_m: 0.8244 - val_precision_m: 0.8351 - val_recall_m: 0.8140\n",
            "Epoch 28/35\n",
            "225/225 [==============================] - 78s 348ms/step - loss: 0.2762 - acc: 0.8766 - f1_m: 0.8775 - precision_m: 0.8683 - recall_m: 0.8874 - val_loss: 0.3801 - val_acc: 0.8336 - val_f1_m: 0.8250 - val_precision_m: 0.8667 - val_recall_m: 0.7871\n",
            "Epoch 29/35\n",
            "225/225 [==============================] - 78s 349ms/step - loss: 0.2684 - acc: 0.8804 - f1_m: 0.8816 - precision_m: 0.8732 - recall_m: 0.8904 - val_loss: 0.3543 - val_acc: 0.8418 - val_f1_m: 0.8452 - val_precision_m: 0.8246 - val_recall_m: 0.8670\n",
            "Epoch 30/35\n",
            "225/225 [==============================] - 79s 349ms/step - loss: 0.2632 - acc: 0.8831 - f1_m: 0.8840 - precision_m: 0.8740 - recall_m: 0.8946 - val_loss: 0.3638 - val_acc: 0.8456 - val_f1_m: 0.8464 - val_precision_m: 0.8389 - val_recall_m: 0.8542\n",
            "Epoch 31/35\n",
            "225/225 [==============================] - 79s 349ms/step - loss: 0.2592 - acc: 0.8853 - f1_m: 0.8864 - precision_m: 0.8766 - recall_m: 0.8966 - val_loss: 0.3572 - val_acc: 0.8469 - val_f1_m: 0.8513 - val_precision_m: 0.8247 - val_recall_m: 0.8796\n",
            "Epoch 32/35\n",
            "225/225 [==============================] - 78s 349ms/step - loss: 0.2519 - acc: 0.8889 - f1_m: 0.8903 - precision_m: 0.8806 - recall_m: 0.9005 - val_loss: 0.3925 - val_acc: 0.8320 - val_f1_m: 0.8264 - val_precision_m: 0.8515 - val_recall_m: 0.8027\n",
            "Epoch 33/35\n",
            "225/225 [==============================] - 79s 352ms/step - loss: 0.2487 - acc: 0.8909 - f1_m: 0.8923 - precision_m: 0.8821 - recall_m: 0.9029 - val_loss: 0.3860 - val_acc: 0.8419 - val_f1_m: 0.8492 - val_precision_m: 0.8092 - val_recall_m: 0.8934\n",
            "Epoch 34/35\n",
            "225/225 [==============================] - 79s 350ms/step - loss: 0.2419 - acc: 0.8943 - f1_m: 0.8957 - precision_m: 0.8853 - recall_m: 0.9065 - val_loss: 0.3687 - val_acc: 0.8501 - val_f1_m: 0.8520 - val_precision_m: 0.8382 - val_recall_m: 0.8664\n",
            "Epoch 35/35\n",
            "225/225 [==============================] - 79s 351ms/step - loss: 0.2355 - acc: 0.8974 - f1_m: 0.8987 - precision_m: 0.8881 - recall_m: 0.9097 - val_loss: 0.3809 - val_acc: 0.8472 - val_f1_m: 0.8502 - val_precision_m: 0.8306 - val_recall_m: 0.8709\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GlMQqMPgISUo"
      },
      "source": [
        "mymodelB.save('/content/gdrive/MyDrive/QuoraQuestions/modelB_last.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MsgHSL8LUu_c",
        "outputId": "4ab91dca-f8dc-4aca-a655-25415a908929"
      },
      "source": [
        "mymodelC, train_historyC, eval_resultsC = train_model('C', batch_size, epochs, input_dim, embedding_matrix, X_train_q1, X_train_q2, Y_train, X_test_q1, X_test_q2, Y_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_2\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_5 (InputLayer)            [(None, 36)]         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_6 (InputLayer)            [(None, 36)]         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "sequential_2 (Sequential)       (None, 128)          4337032     input_5[0][0]                    \n",
            "                                                                 input_6[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "multiply_1 (Multiply)           (None, 128)          0           sequential_2[0][0]               \n",
            "                                                                 sequential_2[1][0]               \n",
            "__________________________________________________________________________________________________\n",
            "flatten_2 (Flatten)             (None, 128)          0           multiply_1[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dense_6 (Dense)                 (None, 128)          16512       flatten_2[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_4 (BatchNor (None, 128)          512         dense_6[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_9 (Dropout)             (None, 128)          0           batch_normalization_4[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "dense_7 (Dense)                 (None, 1)            129         dropout_9[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 4,354,185\n",
            "Trainable params: 240,129\n",
            "Non-trainable params: 4,114,056\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Epoch 1/35\n",
            "225/225 [==============================] - 60s 239ms/step - loss: 0.5527 - acc: 0.7086 - f1_m: 0.7040 - precision_m: 0.7237 - recall_m: 0.6897 - val_loss: 0.6307 - val_acc: 0.5973 - val_f1_m: 0.7071 - val_precision_m: 0.5546 - val_recall_m: 0.9756\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
            "  category=CustomMaskWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/35\n",
            "225/225 [==============================] - 52s 231ms/step - loss: 0.4899 - acc: 0.7567 - f1_m: 0.7472 - precision_m: 0.7781 - recall_m: 0.7200 - val_loss: 0.6032 - val_acc: 0.6589 - val_f1_m: 0.7316 - val_precision_m: 0.6018 - val_recall_m: 0.9333\n",
            "Epoch 3/35\n",
            "225/225 [==============================] - 52s 230ms/step - loss: 0.4658 - acc: 0.7706 - f1_m: 0.7635 - precision_m: 0.7891 - recall_m: 0.7403 - val_loss: 0.4654 - val_acc: 0.7676 - val_f1_m: 0.7662 - val_precision_m: 0.7679 - val_recall_m: 0.7647\n",
            "Epoch 4/35\n",
            "225/225 [==============================] - 52s 231ms/step - loss: 0.4473 - acc: 0.7811 - f1_m: 0.7756 - precision_m: 0.7956 - recall_m: 0.7577 - val_loss: 0.4722 - val_acc: 0.7632 - val_f1_m: 0.7437 - val_precision_m: 0.8067 - val_recall_m: 0.6900\n",
            "Epoch 5/35\n",
            "225/225 [==============================] - 52s 230ms/step - loss: 0.4336 - acc: 0.7880 - f1_m: 0.7835 - precision_m: 0.8002 - recall_m: 0.7683 - val_loss: 0.4297 - val_acc: 0.7869 - val_f1_m: 0.7836 - val_precision_m: 0.7927 - val_recall_m: 0.7749\n",
            "Epoch 6/35\n",
            "225/225 [==============================] - 52s 230ms/step - loss: 0.4209 - acc: 0.7947 - f1_m: 0.7919 - precision_m: 0.8028 - recall_m: 0.7822 - val_loss: 0.5600 - val_acc: 0.6896 - val_f1_m: 0.5833 - val_precision_m: 0.8809 - val_recall_m: 0.4361\n",
            "Epoch 7/35\n",
            "225/225 [==============================] - 52s 230ms/step - loss: 0.4108 - acc: 0.8006 - f1_m: 0.7981 - precision_m: 0.8080 - recall_m: 0.7894 - val_loss: 0.4302 - val_acc: 0.7889 - val_f1_m: 0.7848 - val_precision_m: 0.7970 - val_recall_m: 0.7731\n",
            "Epoch 8/35\n",
            "225/225 [==============================] - 52s 231ms/step - loss: 0.4037 - acc: 0.8049 - f1_m: 0.8031 - precision_m: 0.8096 - recall_m: 0.7977 - val_loss: 0.4212 - val_acc: 0.7932 - val_f1_m: 0.8075 - val_precision_m: 0.7529 - val_recall_m: 0.8707\n",
            "Epoch 9/35\n",
            "225/225 [==============================] - 52s 229ms/step - loss: 0.3900 - acc: 0.8131 - f1_m: 0.8125 - precision_m: 0.8145 - recall_m: 0.8111 - val_loss: 0.6448 - val_acc: 0.7195 - val_f1_m: 0.7682 - val_precision_m: 0.6530 - val_recall_m: 0.9329\n",
            "Epoch 10/35\n",
            "225/225 [==============================] - 52s 229ms/step - loss: 0.3840 - acc: 0.8159 - f1_m: 0.8161 - precision_m: 0.8165 - recall_m: 0.8163 - val_loss: 0.4194 - val_acc: 0.7974 - val_f1_m: 0.7864 - val_precision_m: 0.8284 - val_recall_m: 0.7485\n",
            "Epoch 11/35\n",
            "225/225 [==============================] - 52s 230ms/step - loss: 0.3720 - acc: 0.8233 - f1_m: 0.8236 - precision_m: 0.8213 - recall_m: 0.8264 - val_loss: 0.3998 - val_acc: 0.8090 - val_f1_m: 0.7948 - val_precision_m: 0.8549 - val_recall_m: 0.7426\n",
            "Epoch 12/35\n",
            "225/225 [==============================] - 52s 229ms/step - loss: 0.3644 - acc: 0.8276 - f1_m: 0.8278 - precision_m: 0.8254 - recall_m: 0.8309 - val_loss: 0.3991 - val_acc: 0.8075 - val_f1_m: 0.7903 - val_precision_m: 0.8637 - val_recall_m: 0.7286\n",
            "Epoch 13/35\n",
            "225/225 [==============================] - 52s 230ms/step - loss: 0.3577 - acc: 0.8315 - f1_m: 0.8318 - precision_m: 0.8289 - recall_m: 0.8351 - val_loss: 0.4006 - val_acc: 0.8094 - val_f1_m: 0.8251 - val_precision_m: 0.7601 - val_recall_m: 0.9022\n",
            "Epoch 14/35\n",
            "225/225 [==============================] - 52s 229ms/step - loss: 0.3490 - acc: 0.8363 - f1_m: 0.8372 - precision_m: 0.8331 - recall_m: 0.8420 - val_loss: 0.3785 - val_acc: 0.8194 - val_f1_m: 0.8118 - val_precision_m: 0.8441 - val_recall_m: 0.7819\n",
            "Epoch 15/35\n",
            "225/225 [==============================] - 52s 229ms/step - loss: 0.3411 - acc: 0.8413 - f1_m: 0.8423 - precision_m: 0.8368 - recall_m: 0.8482 - val_loss: 0.3707 - val_acc: 0.8253 - val_f1_m: 0.8239 - val_precision_m: 0.8274 - val_recall_m: 0.8205\n",
            "Epoch 16/35\n",
            "225/225 [==============================] - 52s 230ms/step - loss: 0.3335 - acc: 0.8449 - f1_m: 0.8457 - precision_m: 0.8399 - recall_m: 0.8521 - val_loss: 0.3714 - val_acc: 0.8272 - val_f1_m: 0.8344 - val_precision_m: 0.7981 - val_recall_m: 0.8743\n",
            "Epoch 17/35\n",
            "225/225 [==============================] - 52s 230ms/step - loss: 0.3254 - acc: 0.8497 - f1_m: 0.8510 - precision_m: 0.8440 - recall_m: 0.8585 - val_loss: 0.3626 - val_acc: 0.8321 - val_f1_m: 0.8361 - val_precision_m: 0.8137 - val_recall_m: 0.8597\n",
            "Epoch 18/35\n",
            "225/225 [==============================] - 52s 230ms/step - loss: 0.3184 - acc: 0.8536 - f1_m: 0.8547 - precision_m: 0.8472 - recall_m: 0.8628 - val_loss: 0.3618 - val_acc: 0.8277 - val_f1_m: 0.8346 - val_precision_m: 0.8000 - val_recall_m: 0.8723\n",
            "Epoch 19/35\n",
            "225/225 [==============================] - 52s 229ms/step - loss: 0.3142 - acc: 0.8557 - f1_m: 0.8568 - precision_m: 0.8506 - recall_m: 0.8637 - val_loss: 0.3778 - val_acc: 0.8289 - val_f1_m: 0.8389 - val_precision_m: 0.7898 - val_recall_m: 0.8945\n",
            "Epoch 20/35\n",
            "225/225 [==============================] - 52s 230ms/step - loss: 0.3063 - acc: 0.8597 - f1_m: 0.8612 - precision_m: 0.8534 - recall_m: 0.8695 - val_loss: 0.3651 - val_acc: 0.8332 - val_f1_m: 0.8365 - val_precision_m: 0.8171 - val_recall_m: 0.8571\n",
            "Epoch 21/35\n",
            "225/225 [==============================] - 52s 230ms/step - loss: 0.2992 - acc: 0.8636 - f1_m: 0.8649 - precision_m: 0.8563 - recall_m: 0.8741 - val_loss: 0.3638 - val_acc: 0.8375 - val_f1_m: 0.8425 - val_precision_m: 0.8147 - val_recall_m: 0.8723\n",
            "Epoch 22/35\n",
            "225/225 [==============================] - 52s 230ms/step - loss: 0.2922 - acc: 0.8675 - f1_m: 0.8686 - precision_m: 0.8613 - recall_m: 0.8764 - val_loss: 0.3503 - val_acc: 0.8398 - val_f1_m: 0.8383 - val_precision_m: 0.8428 - val_recall_m: 0.8339\n",
            "Epoch 23/35\n",
            "225/225 [==============================] - 52s 232ms/step - loss: 0.2878 - acc: 0.8699 - f1_m: 0.8713 - precision_m: 0.8641 - recall_m: 0.8790 - val_loss: 0.3704 - val_acc: 0.8368 - val_f1_m: 0.8441 - val_precision_m: 0.8054 - val_recall_m: 0.8869\n",
            "Epoch 24/35\n",
            "225/225 [==============================] - 52s 230ms/step - loss: 0.2798 - acc: 0.8743 - f1_m: 0.8756 - precision_m: 0.8673 - recall_m: 0.8844 - val_loss: 0.3486 - val_acc: 0.8438 - val_f1_m: 0.8478 - val_precision_m: 0.8240 - val_recall_m: 0.8730\n",
            "Epoch 25/35\n",
            "225/225 [==============================] - 52s 230ms/step - loss: 0.2729 - acc: 0.8779 - f1_m: 0.8792 - precision_m: 0.8718 - recall_m: 0.8871 - val_loss: 0.3590 - val_acc: 0.8386 - val_f1_m: 0.8475 - val_precision_m: 0.8002 - val_recall_m: 0.9009\n",
            "Epoch 26/35\n",
            "225/225 [==============================] - 52s 230ms/step - loss: 0.2683 - acc: 0.8804 - f1_m: 0.8817 - precision_m: 0.8735 - recall_m: 0.8903 - val_loss: 0.3455 - val_acc: 0.8466 - val_f1_m: 0.8465 - val_precision_m: 0.8436 - val_recall_m: 0.8496\n",
            "Epoch 27/35\n",
            "225/225 [==============================] - 52s 231ms/step - loss: 0.2619 - acc: 0.8836 - f1_m: 0.8844 - precision_m: 0.8756 - recall_m: 0.8938 - val_loss: 0.3556 - val_acc: 0.8460 - val_f1_m: 0.8435 - val_precision_m: 0.8541 - val_recall_m: 0.8332\n",
            "Epoch 28/35\n",
            "225/225 [==============================] - 52s 229ms/step - loss: 0.2607 - acc: 0.8841 - f1_m: 0.8849 - precision_m: 0.8767 - recall_m: 0.8938 - val_loss: 0.3593 - val_acc: 0.8418 - val_f1_m: 0.8483 - val_precision_m: 0.8123 - val_recall_m: 0.8876\n",
            "Epoch 29/35\n",
            "225/225 [==============================] - 52s 230ms/step - loss: 0.2515 - acc: 0.8889 - f1_m: 0.8900 - precision_m: 0.8820 - recall_m: 0.8983 - val_loss: 0.3476 - val_acc: 0.8477 - val_f1_m: 0.8475 - val_precision_m: 0.8456 - val_recall_m: 0.8494\n",
            "Epoch 30/35\n",
            "225/225 [==============================] - 52s 230ms/step - loss: 0.2485 - acc: 0.8904 - f1_m: 0.8913 - precision_m: 0.8828 - recall_m: 0.9003 - val_loss: 0.3414 - val_acc: 0.8517 - val_f1_m: 0.8520 - val_precision_m: 0.8470 - val_recall_m: 0.8572\n",
            "Epoch 31/35\n",
            "225/225 [==============================] - 52s 230ms/step - loss: 0.2402 - acc: 0.8946 - f1_m: 0.8956 - precision_m: 0.8877 - recall_m: 0.9039 - val_loss: 0.3439 - val_acc: 0.8531 - val_f1_m: 0.8556 - val_precision_m: 0.8381 - val_recall_m: 0.8740\n",
            "Epoch 32/35\n",
            "225/225 [==============================] - 52s 229ms/step - loss: 0.2349 - acc: 0.8967 - f1_m: 0.8978 - precision_m: 0.8893 - recall_m: 0.9066 - val_loss: 0.3590 - val_acc: 0.8495 - val_f1_m: 0.8536 - val_precision_m: 0.8281 - val_recall_m: 0.8807\n",
            "Epoch 33/35\n",
            "225/225 [==============================] - 52s 230ms/step - loss: 0.2310 - acc: 0.8992 - f1_m: 0.9002 - precision_m: 0.8922 - recall_m: 0.9086 - val_loss: 0.3580 - val_acc: 0.8530 - val_f1_m: 0.8550 - val_precision_m: 0.8399 - val_recall_m: 0.8708\n",
            "Epoch 34/35\n",
            "225/225 [==============================] - 52s 230ms/step - loss: 0.2242 - acc: 0.9030 - f1_m: 0.9039 - precision_m: 0.8964 - recall_m: 0.9118 - val_loss: 0.3560 - val_acc: 0.8536 - val_f1_m: 0.8578 - val_precision_m: 0.8309 - val_recall_m: 0.8866\n",
            "Epoch 35/35\n",
            "225/225 [==============================] - 52s 230ms/step - loss: 0.2246 - acc: 0.9022 - f1_m: 0.9031 - precision_m: 0.8943 - recall_m: 0.9123 - val_loss: 0.3569 - val_acc: 0.8529 - val_f1_m: 0.8562 - val_precision_m: 0.8344 - val_recall_m: 0.8793\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rRIIwHwBU9gU",
        "outputId": "06d06ea0-b25a-4c93-dd45-e6d42206858c"
      },
      "source": [
        "mymodelC.save('/content/gdrive/MyDrive/QuoraQuestions/modelC_last.h5')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
            "  category=CustomMaskWarning)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ix0u6a_BZ5rI",
        "outputId": "2eff10a7-9308-40f9-f055-b96d0747ba8c"
      },
      "source": [
        "mymodelD, train_historyD, eval_resultsD = train_model('D', batch_size, epochs, input_dim, embedding_matrix, X_train_q1, X_train_q2, Y_train, X_test_q1, X_test_q2, Y_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, 36)]         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_2 (InputLayer)            [(None, 36)]         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "sequential (Sequential)         (None, 128)          4337032     input_1[0][0]                    \n",
            "                                                                 input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda (Lambda)                 (None, 1)            0           sequential[0][0]                 \n",
            "                                                                 sequential[1][0]                 \n",
            "==================================================================================================\n",
            "Total params: 4,337,032\n",
            "Trainable params: 223,232\n",
            "Non-trainable params: 4,113,800\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Epoch 1/35\n",
            "225/225 [==============================] - 101s 238ms/step - loss: 0.2143 - acc: 0.6634 - f1_m: 0.6756 - precision_m: 0.6289 - recall_m: 0.7416 - val_loss: 0.1967 - val_acc: 0.7036 - val_f1_m: 0.7343 - val_precision_m: 0.6635 - val_recall_m: 0.8223\n",
            "Epoch 2/35\n",
            "225/225 [==============================] - 51s 229ms/step - loss: 0.1711 - acc: 0.7472 - f1_m: 0.7493 - precision_m: 0.7451 - recall_m: 0.7581 - val_loss: 0.1847 - val_acc: 0.7240 - val_f1_m: 0.7652 - val_precision_m: 0.6641 - val_recall_m: 0.9029\n",
            "Epoch 3/35\n",
            "225/225 [==============================] - 52s 229ms/step - loss: 0.1564 - acc: 0.7718 - f1_m: 0.7711 - precision_m: 0.7764 - recall_m: 0.7699 - val_loss: 0.1562 - val_acc: 0.7753 - val_f1_m: 0.7641 - val_precision_m: 0.8010 - val_recall_m: 0.7306\n",
            "Epoch 4/35\n",
            "225/225 [==============================] - 52s 229ms/step - loss: 0.1466 - acc: 0.7879 - f1_m: 0.7856 - precision_m: 0.7958 - recall_m: 0.7777 - val_loss: 0.1481 - val_acc: 0.7854 - val_f1_m: 0.7895 - val_precision_m: 0.7719 - val_recall_m: 0.8080\n",
            "Epoch 5/35\n",
            "225/225 [==============================] - 51s 228ms/step - loss: 0.1405 - acc: 0.7975 - f1_m: 0.7948 - precision_m: 0.8069 - recall_m: 0.7844 - val_loss: 0.1453 - val_acc: 0.7921 - val_f1_m: 0.8002 - val_precision_m: 0.7676 - val_recall_m: 0.8358\n",
            "Epoch 6/35\n",
            "225/225 [==============================] - 52s 230ms/step - loss: 0.1369 - acc: 0.8027 - f1_m: 0.7998 - precision_m: 0.8125 - recall_m: 0.7891 - val_loss: 0.1463 - val_acc: 0.7856 - val_f1_m: 0.7634 - val_precision_m: 0.8477 - val_recall_m: 0.6945\n",
            "Epoch 7/35\n",
            "225/225 [==============================] - 52s 230ms/step - loss: 0.1335 - acc: 0.8082 - f1_m: 0.8049 - precision_m: 0.8194 - recall_m: 0.7924 - val_loss: 0.1359 - val_acc: 0.8044 - val_f1_m: 0.8011 - val_precision_m: 0.8114 - val_recall_m: 0.7914\n",
            "Epoch 8/35\n",
            "225/225 [==============================] - 52s 229ms/step - loss: 0.1301 - acc: 0.8139 - f1_m: 0.8108 - precision_m: 0.8255 - recall_m: 0.7977 - val_loss: 0.1410 - val_acc: 0.7972 - val_f1_m: 0.8096 - val_precision_m: 0.7606 - val_recall_m: 0.8657\n",
            "Epoch 9/35\n",
            "225/225 [==============================] - 52s 230ms/step - loss: 0.1272 - acc: 0.8180 - f1_m: 0.8142 - precision_m: 0.8305 - recall_m: 0.7997 - val_loss: 0.1348 - val_acc: 0.8056 - val_f1_m: 0.8131 - val_precision_m: 0.7800 - val_recall_m: 0.8492\n",
            "Epoch 10/35\n",
            "225/225 [==============================] - 52s 230ms/step - loss: 0.1241 - acc: 0.8236 - f1_m: 0.8202 - precision_m: 0.8376 - recall_m: 0.8044 - val_loss: 0.1291 - val_acc: 0.8155 - val_f1_m: 0.8135 - val_precision_m: 0.8192 - val_recall_m: 0.8081\n",
            "Epoch 11/35\n",
            "225/225 [==============================] - 52s 230ms/step - loss: 0.1218 - acc: 0.8273 - f1_m: 0.8238 - precision_m: 0.8415 - recall_m: 0.8077 - val_loss: 0.1253 - val_acc: 0.8201 - val_f1_m: 0.8131 - val_precision_m: 0.8422 - val_recall_m: 0.7861\n",
            "Epoch 12/35\n",
            "225/225 [==============================] - 52s 230ms/step - loss: 0.1201 - acc: 0.8302 - f1_m: 0.8265 - precision_m: 0.8455 - recall_m: 0.8094 - val_loss: 0.1264 - val_acc: 0.8183 - val_f1_m: 0.8100 - val_precision_m: 0.8451 - val_recall_m: 0.7779\n",
            "Epoch 13/35\n",
            "225/225 [==============================] - 52s 230ms/step - loss: 0.1172 - acc: 0.8349 - f1_m: 0.8311 - precision_m: 0.8502 - recall_m: 0.8133 - val_loss: 0.1291 - val_acc: 0.8150 - val_f1_m: 0.7986 - val_precision_m: 0.8722 - val_recall_m: 0.7367\n",
            "Epoch 14/35\n",
            "225/225 [==============================] - 52s 229ms/step - loss: 0.1155 - acc: 0.8378 - f1_m: 0.8342 - precision_m: 0.8542 - recall_m: 0.8158 - val_loss: 0.1233 - val_acc: 0.8256 - val_f1_m: 0.8266 - val_precision_m: 0.8190 - val_recall_m: 0.8344\n",
            "Epoch 15/35\n",
            "225/225 [==============================] - 52s 230ms/step - loss: 0.1137 - acc: 0.8405 - f1_m: 0.8367 - precision_m: 0.8568 - recall_m: 0.8181 - val_loss: 0.1230 - val_acc: 0.8244 - val_f1_m: 0.8138 - val_precision_m: 0.8625 - val_recall_m: 0.7704\n",
            "Epoch 16/35\n",
            "225/225 [==============================] - 52s 230ms/step - loss: 0.1120 - acc: 0.8431 - f1_m: 0.8392 - precision_m: 0.8601 - recall_m: 0.8198 - val_loss: 0.1211 - val_acc: 0.8277 - val_f1_m: 0.8284 - val_precision_m: 0.8220 - val_recall_m: 0.8350\n",
            "Epoch 17/35\n",
            "225/225 [==============================] - 52s 229ms/step - loss: 0.1102 - acc: 0.8465 - f1_m: 0.8429 - precision_m: 0.8643 - recall_m: 0.8231 - val_loss: 0.1193 - val_acc: 0.8308 - val_f1_m: 0.8278 - val_precision_m: 0.8399 - val_recall_m: 0.8160\n",
            "Epoch 18/35\n",
            "225/225 [==============================] - 52s 230ms/step - loss: 0.1087 - acc: 0.8491 - f1_m: 0.8451 - precision_m: 0.8666 - recall_m: 0.8251 - val_loss: 0.1195 - val_acc: 0.8306 - val_f1_m: 0.8284 - val_precision_m: 0.8361 - val_recall_m: 0.8208\n",
            "Epoch 19/35\n",
            "225/225 [==============================] - 52s 229ms/step - loss: 0.1072 - acc: 0.8512 - f1_m: 0.8477 - precision_m: 0.8694 - recall_m: 0.8275 - val_loss: 0.1197 - val_acc: 0.8303 - val_f1_m: 0.8320 - val_precision_m: 0.8207 - val_recall_m: 0.8438\n",
            "Epoch 20/35\n",
            "225/225 [==============================] - 52s 229ms/step - loss: 0.1060 - acc: 0.8533 - f1_m: 0.8499 - precision_m: 0.8717 - recall_m: 0.8297 - val_loss: 0.1230 - val_acc: 0.8269 - val_f1_m: 0.8319 - val_precision_m: 0.8058 - val_recall_m: 0.8600\n",
            "Epoch 21/35\n",
            "225/225 [==============================] - 52s 230ms/step - loss: 0.1048 - acc: 0.8555 - f1_m: 0.8518 - precision_m: 0.8730 - recall_m: 0.8324 - val_loss: 0.1159 - val_acc: 0.8368 - val_f1_m: 0.8322 - val_precision_m: 0.8529 - val_recall_m: 0.8126\n",
            "Epoch 22/35\n",
            "225/225 [==============================] - 52s 231ms/step - loss: 0.1030 - acc: 0.8582 - f1_m: 0.8544 - precision_m: 0.8768 - recall_m: 0.8336 - val_loss: 0.1217 - val_acc: 0.8291 - val_f1_m: 0.8318 - val_precision_m: 0.8163 - val_recall_m: 0.8480\n",
            "Epoch 23/35\n",
            "225/225 [==============================] - 52s 230ms/step - loss: 0.1016 - acc: 0.8606 - f1_m: 0.8573 - precision_m: 0.8796 - recall_m: 0.8366 - val_loss: 0.1146 - val_acc: 0.8391 - val_f1_m: 0.8355 - val_precision_m: 0.8514 - val_recall_m: 0.8202\n",
            "Epoch 24/35\n",
            "225/225 [==============================] - 52s 230ms/step - loss: 0.1002 - acc: 0.8627 - f1_m: 0.8594 - precision_m: 0.8814 - recall_m: 0.8390 - val_loss: 0.1142 - val_acc: 0.8390 - val_f1_m: 0.8358 - val_precision_m: 0.8494 - val_recall_m: 0.8227\n",
            "Epoch 25/35\n",
            "225/225 [==============================] - 52s 229ms/step - loss: 0.0987 - acc: 0.8656 - f1_m: 0.8622 - precision_m: 0.8853 - recall_m: 0.8406 - val_loss: 0.1133 - val_acc: 0.8412 - val_f1_m: 0.8423 - val_precision_m: 0.8335 - val_recall_m: 0.8514\n",
            "Epoch 26/35\n",
            "225/225 [==============================] - 52s 230ms/step - loss: 0.0982 - acc: 0.8665 - f1_m: 0.8633 - precision_m: 0.8852 - recall_m: 0.8428 - val_loss: 0.1150 - val_acc: 0.8383 - val_f1_m: 0.8289 - val_precision_m: 0.8767 - val_recall_m: 0.7861\n",
            "Epoch 27/35\n",
            "225/225 [==============================] - 51s 229ms/step - loss: 0.0983 - acc: 0.8663 - f1_m: 0.8629 - precision_m: 0.8859 - recall_m: 0.8415 - val_loss: 0.1131 - val_acc: 0.8420 - val_f1_m: 0.8323 - val_precision_m: 0.8829 - val_recall_m: 0.7873\n",
            "Epoch 28/35\n",
            "225/225 [==============================] - 51s 229ms/step - loss: 0.0961 - acc: 0.8699 - f1_m: 0.8667 - precision_m: 0.8899 - recall_m: 0.8451 - val_loss: 0.1111 - val_acc: 0.8449 - val_f1_m: 0.8366 - val_precision_m: 0.8803 - val_recall_m: 0.7972\n",
            "Epoch 29/35\n",
            "225/225 [==============================] - 51s 228ms/step - loss: 0.0947 - acc: 0.8724 - f1_m: 0.8688 - precision_m: 0.8923 - recall_m: 0.8470 - val_loss: 0.1111 - val_acc: 0.8438 - val_f1_m: 0.8380 - val_precision_m: 0.8670 - val_recall_m: 0.8109\n",
            "Epoch 30/35\n",
            "225/225 [==============================] - 52s 229ms/step - loss: 0.0938 - acc: 0.8737 - f1_m: 0.8703 - precision_m: 0.8940 - recall_m: 0.8482 - val_loss: 0.1118 - val_acc: 0.8427 - val_f1_m: 0.8333 - val_precision_m: 0.8826 - val_recall_m: 0.7894\n",
            "Epoch 31/35\n",
            "225/225 [==============================] - 52s 229ms/step - loss: 0.0926 - acc: 0.8759 - f1_m: 0.8726 - precision_m: 0.8954 - recall_m: 0.8514 - val_loss: 0.1108 - val_acc: 0.8449 - val_f1_m: 0.8393 - val_precision_m: 0.8675 - val_recall_m: 0.8130\n",
            "Epoch 32/35\n",
            "225/225 [==============================] - 52s 229ms/step - loss: 0.0920 - acc: 0.8770 - f1_m: 0.8737 - precision_m: 0.8965 - recall_m: 0.8524 - val_loss: 0.1097 - val_acc: 0.8476 - val_f1_m: 0.8437 - val_precision_m: 0.8629 - val_recall_m: 0.8254\n",
            "Epoch 33/35\n",
            "225/225 [==============================] - 51s 228ms/step - loss: 0.0908 - acc: 0.8790 - f1_m: 0.8760 - precision_m: 0.8991 - recall_m: 0.8543 - val_loss: 0.1101 - val_acc: 0.8462 - val_f1_m: 0.8428 - val_precision_m: 0.8586 - val_recall_m: 0.8278\n",
            "Epoch 34/35\n",
            "225/225 [==============================] - 51s 229ms/step - loss: 0.0893 - acc: 0.8814 - f1_m: 0.8783 - precision_m: 0.9017 - recall_m: 0.8563 - val_loss: 0.1106 - val_acc: 0.8461 - val_f1_m: 0.8464 - val_precision_m: 0.8417 - val_recall_m: 0.8513\n",
            "Epoch 35/35\n",
            "225/225 [==============================] - 52s 230ms/step - loss: 0.0889 - acc: 0.8822 - f1_m: 0.8794 - precision_m: 0.9020 - recall_m: 0.8582 - val_loss: 0.1082 - val_acc: 0.8491 - val_f1_m: 0.8433 - val_precision_m: 0.8737 - val_recall_m: 0.8151\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jnXZ5t8DaAHI"
      },
      "source": [
        "mymodelD.save('/content/gdrive/MyDrive/QuoraQuestions/modelD_last.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DAs3zZF6k-qs"
      },
      "source": [
        "# load data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d4CCiVUcZoDo"
      },
      "source": [
        "import pickle\n",
        "with open('/content/gdrive/MyDrive/QuoraQuestions/mytokenizer.pickle', 'rb') as handle:\n",
        "    loadedtokenizer = pickle.load(handle)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VqOxDPGQUOgA"
      },
      "source": [
        "loadedtokenizer.texts_to_sequences(q1_train[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UFtQv1mjZ-Ll"
      },
      "source": [
        "def preprocessing_with_loaded_pickle(question1_train_list, question2_train_list, Y_train_list, question1_test_list, question2_test_list, Y_test_list, tokenizer):\n",
        "  X_train_q1 = tokenizer.texts_to_sequences(question1_train_list)\n",
        "  X_train_q1 = pad_sequences(X_train_q1, maxlen = input_length, padding='post')\n",
        "\n",
        "  X_train_q2 = tokenizer.texts_to_sequences(question2_train_list)\n",
        "  X_train_q2 = pad_sequences(X_train_q2, maxlen = input_length, padding='post')\n",
        "\n",
        "  word_index = tokenizer.word_index\n",
        "  input_dim = len(word_index)+1\n",
        "  embedding_matrix = np.random.random((input_dim, output_dim))\n",
        "  for word, i in word_index.items():\n",
        "      embedding_vector = embeddings_dict.get(word)\n",
        "      if embedding_vector is not None:\n",
        "          embedding_matrix[i] = embedding_vector\n",
        "  \n",
        "  Y_train = np.asarray(Y_train_list)\n",
        "  Y_test = np.asarray(Y_test_list)\n",
        "  \n",
        "  X_test_q1 = tokenizer.texts_to_sequences(question1_test_list)\n",
        "  X_test_q1 = pad_sequences(X_test_q1, maxlen = input_length, padding='post')\n",
        "\n",
        "  X_test_q2 = tokenizer.texts_to_sequences(question2_test_list)\n",
        "  X_test_q2 = pad_sequences(X_test_q2, maxlen = input_length, padding='post')\n",
        "\n",
        "  return input_dim, embedding_matrix, X_train_q1, X_train_q2, Y_train, X_test_q1, X_test_q2, Y_test, tokenizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7zLD5R1EaI9I"
      },
      "source": [
        "input_dim, embedding_matrix, X_train_q1, X_train_q2, Y_train, X_test_q1, X_test_q2, Y_test, my_loaded_tokenizer = preprocessing_with_loaded_pickle(q1_train, q2_train, y_train, q1_val, q2_val, y_val, loadedtokenizer)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vlsIiIDVeGbx"
      },
      "source": [
        "# testin model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aeD1uNb8Xyf9"
      },
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_curve, auc, roc_auc_score, log_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GrJwTLUUP5k_"
      },
      "source": [
        "modelA_loaded = tf.keras.models.load_model('/content/gdrive/MyDrive/QuoraQuestions/modelA_last.h5', \n",
        "                                           custom_objects={'f1_m':f1_m, 'precision_m':precision_m, \"recall_m\":recall_m})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P9ilUuuvRc9Z",
        "outputId": "adb0b255-803d-430d-f363-d132e0948897"
      },
      "source": [
        "# compile the model\n",
        "modelA_loaded.compile(optimizer='adam', loss = 'binary_crossentropy', metrics=['acc',f1_m,precision_m, recall_m])\n",
        "\n",
        "# evaluate the model\n",
        "loss, accuracy, f1_score, precision, recall = modelA_loaded.evaluate([X_test_q1, X_test_q2], Y_test)\n",
        "\n",
        "preds = modelA_loaded.predict([X_test_q1, X_test_q2])\n",
        "roc_auc = roc_auc_score(Y_test, preds)\n",
        "\n",
        "print(\"modelA\")\n",
        "print('Accuracy: %f' % accuracy)\n",
        "print('Precision: %f' % precision)\n",
        "print('Recall: %f' % recall)\n",
        "print('F1 Score: %f' % f1_score)\n",
        "print('AUC-ROC: %f' % roc_auc)\n",
        "print('Log Loss: %f' % loss)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1594/1594 [==============================] - 47s 27ms/step - loss: 0.3646 - acc: 0.8542 - f1_m: 0.8570 - precision_m: 0.8211 - recall_m: 0.9044\n",
            "modelA\n",
            "Accuracy: 0.854217\n",
            "Precision: 0.821111\n",
            "Recall: 0.904430\n",
            "F1 Score: 0.857011\n",
            "AUC-ROC: 0.937873\n",
            "Log Loss: 0.364571\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YncNsfbHRli_"
      },
      "source": [
        "modelB_loaded = tf.keras.models.load_model('/content/gdrive/MyDrive/QuoraQuestions/modelB_last.h5', \n",
        "                                           custom_objects={'f1_m':f1_m, 'precision_m':precision_m, \"recall_m\":recall_m})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rJogAFnDT1Zf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ac4ea775-7d26-4bfc-cde4-20685f9e4e2b"
      },
      "source": [
        "# compile the model\n",
        "modelB_loaded.compile(optimizer='adam', loss = 'binary_crossentropy', metrics=['acc',f1_m,precision_m, recall_m])\n",
        "\n",
        "# evaluate the model\n",
        "loss, accuracy, f1_score, precision, recall = modelB_loaded.evaluate([X_test_q1, X_test_q2], Y_test)\n",
        "\n",
        "preds_B = modelB_loaded.predict([X_test_q1, X_test_q2])\n",
        "\n",
        "roc_auc = roc_auc_score(Y_test, preds_B)\n",
        "\n",
        "print(\"modelB\")\n",
        "print('Accuracy: %f' % accuracy)\n",
        "print('Precision: %f' % precision)\n",
        "print('Recall: %f' % recall)\n",
        "print('F1 Score: %f' % f1_score)\n",
        "print('AUC-ROC: %f' % roc_auc)\n",
        "print('Log Loss: %f' % loss)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1594/1594 [==============================] - 47s 27ms/step - loss: 0.3809 - acc: 0.8472 - f1_m: 0.8461 - precision_m: 0.8307 - recall_m: 0.8708\n",
            "modelB\n",
            "Accuracy: 0.847156\n",
            "Precision: 0.830691\n",
            "Recall: 0.870764\n",
            "F1 Score: 0.846148\n",
            "AUC-ROC: 0.929831\n",
            "Log Loss: 0.380886\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TJDdyQixbXHw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bddc9dc7-a9de-45b7-ab99-82e0f37cb215"
      },
      "source": [
        "modelC_loaded = tf.keras.models.load_model('/content/gdrive/MyDrive/QuoraQuestions/modelC_last.h5', \n",
        "                                           custom_objects={'f1_m':f1_m, 'precision_m':precision_m, \"recall_m\":recall_m})\n",
        "# compile the model\n",
        "modelC_loaded.compile(optimizer='adam', loss = 'binary_crossentropy', metrics=['acc',f1_m,precision_m, recall_m])\n",
        "\n",
        "# evaluate the model\n",
        "loss, accuracy, f1_score, precision, recall = modelC_loaded.evaluate([X_test_q1, X_test_q2], Y_test)\n",
        "\n",
        "preds_C = modelC_loaded.predict([X_test_q1, X_test_q2])\n",
        "\n",
        "roc_auc = roc_auc_score(Y_test, preds_C)\n",
        "\n",
        "print(\"modelC\")\n",
        "print('Accuracy: %f' % accuracy)\n",
        "print('Precision: %f' % precision)\n",
        "print('Recall: %f' % recall)\n",
        "print('F1 Score: %f' % f1_score)\n",
        "print('AUC-ROC: %f' % roc_auc)\n",
        "print('Log Loss: %f' % loss)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1594/1594 [==============================] - 29s 16ms/step - loss: 0.3569 - acc: 0.8529 - f1_m: 0.8526 - precision_m: 0.8348 - recall_m: 0.8796\n",
            "modelC\n",
            "Accuracy: 0.852942\n",
            "Precision: 0.834753\n",
            "Recall: 0.879597\n",
            "F1 Score: 0.852591\n",
            "AUC-ROC: 0.935025\n",
            "Log Loss: 0.356903\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ij-74nR2FNUy"
      },
      "source": [
        "modelD_loaded = mymodelD"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WgK2i6oDcA0l",
        "outputId": "b462582c-cb22-4f51-dbe3-e770503e0459"
      },
      "source": [
        "# evaluate the model\n",
        "loss, accuracy, f1_score, precision, recall = modelD_loaded.evaluate([X_test_q1, X_test_q2], Y_test)\n",
        "\n",
        "preds_D = modelD_loaded.predict([X_test_q1, X_test_q2])\n",
        "\n",
        "roc_auc = roc_auc_score(Y_test, preds_D)\n",
        "\n",
        "print(\"modelD\")\n",
        "print('Accuracy: %f' % accuracy)\n",
        "print('Precision: %f' % precision)\n",
        "print('Recall: %f' % recall)\n",
        "print('F1 Score: %f' % f1_score)\n",
        "print('AUC-ROC: %f' % roc_auc)\n",
        "print('Log Loss: %f' % loss)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1594/1594 [==============================] - 26s 16ms/step - loss: 0.1082 - acc: 0.8491 - f1_m: 0.8387 - precision_m: 0.8737 - recall_m: 0.8147\n",
            "modelD\n",
            "Accuracy: 0.849098\n",
            "Precision: 0.873697\n",
            "Recall: 0.814733\n",
            "F1 Score: 0.838711\n",
            "AUC-ROC: 0.925007\n",
            "Log Loss: 0.108170\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N9DMTLO-rvQb"
      },
      "source": [
        "#Cross Validation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GLCei9gGrxPS"
      },
      "source": [
        "from sklearn.model_selection import KFold\n",
        "\n",
        "def get_list(ori_list, index_list):\n",
        "  return [ori_list[i] for i in index_list]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UEiNiib3r5L-",
        "outputId": "d0ce57fd-b409-404f-8fb9-7cc7a1de9e3c"
      },
      "source": [
        "X_train_q1 = train_df['question1_cleaned'].astype(str).tolist()\n",
        "X_train_q2 = train_df['question2_cleaned'].astype(str).tolist()\n",
        "Y_train = train_df['is_duplicate'].astype(int).tolist()\n",
        "kfold = KFold(n_splits=5, shuffle=True)\n",
        "\n",
        "fold_no = 0\n",
        "acc_per_fold = []\n",
        "loss_per_fold = []\n",
        "for train, test in kfold.split(X_train_q1, X_train_q2, Y_train):\n",
        "\n",
        "  q1_train = get_list(X_train_q1,train)\n",
        "  q2_train = get_list(X_train_q2,train)\n",
        "  y_train = get_list(Y_train,train)\n",
        "\n",
        "  q1_val = get_list(X_train_q1,test)\n",
        "  q2_val = get_list(X_train_q2, test)\n",
        "  y_val = get_list(Y_train, test)\n",
        "\n",
        "  print('------------------------------------------------------------------------')\n",
        "  print(f'Training for fold {fold_no} ...')\n",
        "\n",
        "  mymodel, train_history, eval_results = train_model(q1_train, q2_train, y_train, q1_val, q2_val, y_val, batch_size, epochs, 'A')\n",
        "\n",
        "  # Generate generalization metrics\n",
        "  print(f'Score for fold {fold_no}: {mymodel.metrics_names[0]} of {eval_results[0]}; {mymodel.metrics_names[1]} of {eval_results[1]*100}%')\n",
        "  acc_per_fold.append(eval_results[1] * 100)\n",
        "  loss_per_fold.append(eval_results[0])\n",
        "\n",
        "  # Increase fold number\n",
        "  fold_no = fold_no + 1\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['b' 'c' 'd' 'e']\n",
            "['a']\n",
            "['a' 'b' 'c' 'e']\n",
            "['d']\n",
            "['a' 'b' 'd' 'e']\n",
            "['c']\n",
            "['a' 'b' 'c' 'd']\n",
            "['e']\n",
            "['a' 'c' 'd' 'e']\n",
            "['b']\n"
          ]
        }
      ]
    }
  ]
}