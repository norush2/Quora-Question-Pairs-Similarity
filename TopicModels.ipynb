{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TopicModels.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K-xAXYSnkdLm"
      },
      "source": [
        "# Visualising word cloud"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "snoUH2b7kBhQ"
      },
      "source": [
        "!pip -q install yellowbrick==1.3\n",
        "!pip -q install kneed\n",
        "!pip install fuzzywuzzy\n",
        "!pip install python-Levenshtein\n",
        "!pip install plotly\n",
        "!pip install squarify\n",
        "!pip install chart_studio\n",
        "!pip -q install factor_analyzer\n",
        "! pip install distance\n",
        "!pip install sentence-transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gOy-K1pzkDBq"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from scipy.stats import f_oneway\n",
        "import statsmodels.api as sm\n",
        "from statsmodels.formula.api import ols\n",
        "import plotly\n",
        "import plotly.express as px\n",
        "import plotly.io as pio\n",
        "import datetime\n",
        "import squarify\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EaD6b5fdkBjs"
      },
      "source": [
        "# Pre-processing data\n",
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer \n",
        "from nltk.corpus import wordnet\n",
        "from nltk.corpus import stopwords\n",
        "import functools\n",
        "from functools import lru_cache\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "\n",
        "# Word embedding model Word2Vec\n",
        "import gensim\n",
        "import gensim.downloader as gensim_api\n",
        "from gensim.models.word2vec import Word2Vec\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Bag of words TD-IDF\n",
        "from sklearn import feature_extraction, model_selection, naive_bayes, pipeline, manifold, preprocessing\n",
        "\n",
        "# Advanced feature extraction\n",
        "from fuzzywuzzy import fuzz\n",
        "from fuzzywuzzy import process\n",
        "\n",
        "# import data\n",
        "from google.colab import files\n",
        "import openpyxl\n",
        "\n",
        "pio.renderers.default = 'colab' \n",
        "import distance\n",
        "\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9X4-q3g3kBmI"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive', force_remount=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mBzUvhagkBoV"
      },
      "source": [
        "cleaned_features = pd.read_csv(\"/content/gdrive/MyDrive/CS3244 Machine Learning/Data/cleaned_features.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0evth2RvkFxD"
      },
      "source": [
        "cleaned_features.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fmd3zWH4kBqS"
      },
      "source": [
        "subsample = cleaned_features.sample(n=100000, random_state = 3244)\n",
        "subsample[[\"is_duplicate\"]].value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UMWyelS6kHBX"
      },
      "source": [
        "word_dict1 = {}\n",
        "for i in subsample[\"question1_cleaned\"]:\n",
        "  word_array = i.split()\n",
        "  for j in word_array:\n",
        "    if j not in word_dict1.keys():\n",
        "      word_dict1[j] = 1\n",
        "    else:\n",
        "      word_dict1[j] += 1\n",
        "#sorted(word_dict1.items(), key=lambda x: x[1], reverse = True)[:20]\n",
        "sorted(word_dict1.items(), key=lambda x: x[1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RfrdaQNrkBsf"
      },
      "source": [
        "word_dict2 = {}\n",
        "for i in subsample[\"question2_cleaned\"]:\n",
        "  word_array = i.split()\n",
        "  for j in word_array:\n",
        "    if j not in word_dict2.keys():\n",
        "      word_dict2[j] = 1\n",
        "    else:\n",
        "      word_dict2[j] += 1\n",
        "sorted(word_dict2.items(), key=lambda x: x[1], reverse = True)[:20]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QUB-8srRkBvF"
      },
      "source": [
        "# look at the bottom few words\n",
        "sorted(word_dict1.items(), key=lambda x: x[1])[:20]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dn4OeGCPkMJi"
      },
      "source": [
        "stopwords_lemmatize = pd.read_csv(\"/content/gdrive/MyDrive/CS3244 Machine Learning/Data/stopwords_lemmatize_features.csv\")\n",
        "stopwords_lemmatize.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QDumQzDmkBxb"
      },
      "source": [
        "subsample_stopwords_lemmatize = stopwords_lemmatize.sample(n=100000, random_state = 3244)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mRYgevPwkTBh"
      },
      "source": [
        "# visualise the word cloud\n",
        "from wordcloud import WordCloud\n",
        "long_string1 = ','.join(list(subsample_stopwords_lemmatize['question1_stopwords_lemmatize'].values))\n",
        "long_string1\n",
        "wordcloud1 = WordCloud(background_color=\"white\", max_words=5000, contour_width=3, contour_color='steelblue', random_state = 3244)\n",
        "\n",
        "wordcloud1.generate(long_string1)\n",
        "wordcloud1.to_image()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "21HnBfT0kTDW"
      },
      "source": [
        "long_string2 = ','.join(list(subsample_stopwords_lemmatize['question2_stopwords_lemmatize'].values))\n",
        "long_string2\n",
        "wordcloud2 = WordCloud(background_color=\"white\", max_words=5000, contour_width=3, contour_color='steelblue', random_state = 3244)\n",
        "\n",
        "wordcloud2.generate(long_string2)\n",
        "wordcloud2.to_image()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wRKhUSU4kWWn"
      },
      "source": [
        "wordcloud1_remove_collocations = WordCloud(background_color=\"white\", max_words=5000, contour_width=3, contour_color='steelblue', random_state = 3244, collocations = False)\n",
        "\n",
        "wordcloud1_remove_collocations.generate(long_string1)\n",
        "wordcloud1_remove_collocations.to_image()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dnJ__P4EkWYR"
      },
      "source": [
        "wordcloud2_remove_collocations = WordCloud(background_color=\"white\", max_words=5000, contour_width=3, contour_color='steelblue', random_state = 3244, collocations = False)\n",
        "\n",
        "wordcloud2_remove_collocations.generate(long_string2)\n",
        "wordcloud2_remove_collocations.to_image()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BWK-ytqykTFb"
      },
      "source": [
        "wordcloud1_df = pd.DataFrame(wordcloud1_remove_collocations.words_.items())\n",
        "# wordcloud1_df.head(10)\n",
        "wordcloud1_df.tail(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GT-7tB9QkTHG"
      },
      "source": [
        "wordcloud2_df = pd.DataFrame(wordcloud2_remove_collocations.words_.items())\n",
        "# wordcloud2_df.head(10)\n",
        "wordcloud2_df.tail(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bj39SZDokkRy"
      },
      "source": [
        "# Topic Modelling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T1lBy6COklXI"
      },
      "source": [
        "from gensim import corpora, models"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IonkiUyyklZN"
      },
      "source": [
        "list_of_list_of_tokens = []\n",
        "for i in subsample_stopwords_lemmatize[\"question1_stopwords_lemmatize\"]:\n",
        "  list_of_list_of_tokens.append(i.split())\n",
        "\n",
        "dictionary_LDA = corpora.Dictionary(list_of_list_of_tokens)\n",
        "dictionary_LDA.filter_extremes(no_below=3)\n",
        "corpus = [dictionary_LDA.doc2bow(list_of_tokens) for list_of_tokens in list_of_list_of_tokens]\n",
        "num_topics = 20\n",
        "\n",
        "lda_model = models.LdaModel(corpus, num_topics=num_topics, \\\n",
        "                                  id2word=dictionary_LDA, \\\n",
        "                                  passes=4, alpha=[0.01]*num_topics, \\\n",
        "                                  eta=[0.01]*len(dictionary_LDA.keys()))\n",
        "\n",
        "for i,topic in lda_model.show_topics(formatted=True, num_topics=num_topics, num_words=10):\n",
        "    print(str(i)+\": \"+ topic)\n",
        "    print()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O_MZkQI8kla9"
      },
      "source": [
        "list_of_list_of_tokens2 = []\n",
        "for i in subsample_stopwords_lemmatize[\"question2_stopwords_lemmatize\"]:\n",
        "  list_of_list_of_tokens2.append(i.split())\n",
        "\n",
        "dictionary_LDA2 = corpora.Dictionary(list_of_list_of_tokens2)\n",
        "dictionary_LDA2.filter_extremes(no_below=3)\n",
        "corpus2 = [dictionary_LDA2.doc2bow(list_of_tokens) for list_of_tokens in list_of_list_of_tokens2]\n",
        "\n",
        "lda_model2 = models.LdaModel(corpus2, num_topics=num_topics, \\\n",
        "                                  id2word=dictionary_LDA2, \\\n",
        "                                  passes=4, alpha=[0.01]*num_topics, \\\n",
        "                                  eta=[0.01]*len(dictionary_LDA2.keys()))\n",
        "\n",
        "for i,topic in lda_model2.show_topics(formatted=True, num_topics=num_topics, num_words=10):\n",
        "    print(str(i)+\": \"+ topic)\n",
        "    print()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EkfQ-3cRkldB"
      },
      "source": [
        "# corpus[0] means the first document\n",
        "lda_model[corpus[0]] \n",
        "\n",
        "subsample_stopwords_lemmatize[\"question1_stopwords_lemmatize\"].iloc[0]\n",
        "test = 'safety precaution handle shotgun propose nra massachisetts'.split()\n",
        "lda_model[dictionary_LDA.doc2bow(test)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9zlOFBDYkpWA"
      },
      "source": [
        "subsample_stopwords_lemmatize[\"question2_stopwords_lemmatize\"].iloc[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tg7HSIflkpXr"
      },
      "source": [
        "lda_model[corpus[1]]"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}